# 开源GUI智能体曙光乍现：ShowUI图形用户界面轻量级视觉-语言-动作模型深度解读

如今图形用户界面（GUIs）在生活中很关键，但多数智能体依赖文本元信息感知 UI 有局限，所以需要像人一样感知交互 UI 的视觉智能体。本文的重要意义在于开发了 ShowUI 模型，用创新方法处理视觉建模、模态交互及数据问题，轻量级模型和少量数据就实现出色的定位与导航能力，推动 GUI 智能体发展，有望更好地提升人类工作效率、优化人机交互体验。

## 摘要
构建图形用户界面（GUI）助手对于提高人类工作流程的生产力具有重要意义。虽然大多数智能体是基于语言的，依赖具有丰富文本元信息（如 HTML 或辅助功能树）的闭源 API，但它们在像人类一样感知 UI 视觉方面存在局限性，这凸显了对 GUI 视觉智能体的需求。在这项工作中，我们开发了一种数字世界中的视觉 - 语言 - 动作模型——ShowUI，它具有以下创新点：（i）UI 引导的视觉标记选择，通过将屏幕截图构建为 UI 连接图，自适应地识别其冗余关系，并在自注意力模块中作为标记选择的标准，从而降低计算成本；（ii）交错的视觉 - 语言 - 动作流，灵活地统一 GUI 任务中的各种需求，能够有效管理导航中的视觉 - 动作历史，或者为每个屏幕截图配对多轮查询 - 动作序列以提高训练效率；（iii）小规模高质量 GUI 指令跟随数据集，通过精心的数据整理和采用重采样策略来解决显著的数据类型不平衡问题。凭借上述组件，使用 256K 数据的轻量级 2B 模型 ShowUI 在零样本屏幕截图定位中达到了 75.1% 的高准确率。其 UI 引导的标记选择在训练期间进一步减少了 33% 的冗余视觉标记，并将性能提升了 1.4 倍。在网络[12]、移动[36]和在线[40]环境中的导航实验进一步强调了我们的模型在推进 GUI 视觉智能体方面的有效性和潜力。模型可在 https://github.com/showlab/ShowUI 获取。

### 研究背景
随着图形用户界面（GUIs）在日常数字交互中占据核心地位，以及大语言模型（LLMs）展现出构建智能体处理复杂任务的潜力，GUI 自动化成为研究热点。然而，早期基于语言的智能体依赖闭源 API 和文本元信息，在感知 UI 视觉上与人类差异大，难以满足实际需求。开发能像人类一样感知和交互 UI 的视觉智能体迫在眉睫，但面临诸多挑战。如高分辨率 UI 截图带来视觉建模计算成本高昂难题，GUI 动作因设备差异和复杂交互模式难以有效建模，不同设备的 GUI 数据多样且不平衡致使高质量训练数据选择困难，这些挑战制约了 GUI 视觉智能体发展，亟待解决。

### 技术创新
1. **UI 引导的视觉标记选择**：通过将屏幕截图构建为 UI 连接图，依据 RGB 值识别补丁冗余关系，以此指导视觉编码器和语言模型自注意力模块进行标记选择，减少计算量，在训练中大幅降低冗余视觉标记数量并显著提升训练速度。
2. **交错的视觉 - 语言 - 动作流**：以 JSON 格式规范动作并记录动作空间辅助模型理解，创新地结合动作与视觉导航历史及多轮动作与文本查询，构建交错流框架，统一 GUI 任务需求，提高训练效率及导航性能，有效处理复杂模态交互。
3. **小规模高质量指令跟随数据集**：深入剖析网络、桌面、移动等 GUI 数据特性，精心筛选并构建数据集，运用重采样策略平衡数据类型差异，以少量优质数据支撑模型训练，解决数据不平衡问题，提升模型泛化能力和性能稳定性。

### 技术实现
1. **视觉建模**：将屏幕截图划分为补丁构建 UI 连接图，基于图中连接组件情况为自注意力模块确定标记选择策略，训练时随机跳过部分冗余标记，保留关键位置信息，实现高效视觉处理且无额外参数引入，推理时灵活选择是否使用标记选择。
2. **动作管理**：把动作标准化为 JSON 格式并提供动作空间说明文档，导航时构建交错视觉 - 动作流记录历史信息，依设备特性和任务需求决定视觉历史保留与掩码策略；处理动作与文本查询时采用多轮交互模式，为单张截图预测多个动作注释，优化数据利用。
3. **数据处理**：网络数据注重采集视觉元素丰富的部分，过滤静态文本；桌面数据挖掘 OmniAct 并借助 GPT - 4o 生成多样化查询扩充；移动数据选取功能描述佳的数据。同时，为平衡不同规模和类型数据集，设置等概率采样策略保障各数据类型在训练中有效利用。

### 实验结果
1. **定位任务**：在 Screenspot 基准测试中，ShowUI 以 2B 轻量级模型和 256K 数据达到 75.1% 零样本屏幕截图定位准确率，超越众多模型。其中文本定位能力强且跨平台转移性佳，混合导航数据不损性能，凸显其在定位任务优势，证明模型对 UI 元素识别精准高效。
2. **导航任务**
    - **移动环境（AITW）**：有无视觉历史对比表明视觉上下文至关重要，有视觉历史时 ShowUI 准确率显著提升。零样本导航可转移性强，且优于依赖闭源或 HTML 信息的模型，有力验证其在移动设备复杂导航场景的适应与处理能力。
    - **网络环境（Mind2Web）**：指令调整显著提升性能，零样本性能可比肩预训练微调模型，虽此环境视觉重要性稍低，但揭示跨网站和跨域视觉感知瓶颈，为后续优化指明方向。
    - **在线环境（MiniWob）**：尽管环境简单，但 ShowUI 零样本性能与微调模型差距大，暴露离线指令调整局限，明确需开发在线学习策略以应对新情况，完善模型性能。
3. **消融研究**
    - **UI 引导标记选择**：多种方法对比证实基于 UI 图的标记选择平衡准确率与速度最优，虽测试时因分辨率损失有轻微准确率波动，但整体可靠，确定了该方法在视觉建模优化的关键作用及合适参数选择。
    - **交错流组件**：动作 - 查询流提升定位训练效率和性能；动作 - 视觉流证实视觉历史助于导航，交错流整体为有效多模态交互策略，为模型架构设计合理性提供有力支撑。
    - **指令调整数据**：数据质量主导模型表现，如 OmniAct 经扩充后效果佳；筛选后的网络数据优于大规模同类数据，平衡采样策略稳定提升准确率，彰显数据处理各环节对模型性能的关键影响及优化成效。

## 1. 引言
图形用户界面（GUIs）是个人与数字世界互动的核心，作为一系列日常活动的虚拟具身接口。与此同时，大语言模型（LLMs）[32]凭借其理解复杂语言指令和无缝集成工具的能力，在通过构建智能体执行复杂任务方面展现出巨大潜力[1, 13, 16, 56]。这一进展激发了智能 GUI 智能体的开发，这些智能体可以根据用户意图显著简化人类工作流程。

图 1. ShowUI 是用于 GUI 自动化的视觉 - 语言 - 动作模型。给定环境屏幕截图，ShowUI 使用 UI 引导的标记选择进行视觉建模，并在循环内输出交互动作。

GUI 自动化的早期努力主要集中在开发语言智能体[12, 47, 55]，这些智能体依赖像 GPT - 4[32]这样基于闭源 API 的大语言模型。这些智能体利用 HTML 输入或辅助功能树等丰富的文本元数据来执行导航和其他任务。然而，纯文本方法在现实世界应用中存在局限性，在现实应用中用户通常通过屏幕截图以视觉方式与用户界面交互，无法访问底层结构的权威信息。这一局限性凸显了开发能够像人类一样感知和交互 UI 的 GUI 视觉智能体的必要性，例如协助创建幻灯片[29]。

图 2. 左：ShowUI 与其他 GUI 视觉模型在模型大小和训练规模（面积）方面的零样本屏幕点定位比较；ShowUI 以更小的训练数据集（256K）达到了最先进的准确率，并且是最轻量化的模型（2B）。右：基于 Qwen2 - VL - 2B[41]，我们的 UI 引导视觉标记选择在训练期间减少了 33% 的视觉标记冗余，实现了 1.4 倍的加速。

然而，与自然图像处理相比，GUI 视觉感知带来了独特的挑战，需要诸如 UI 元素定位或动作执行等专门技能，而不是多模态聊天机器人典型的对话能力。认识到这一差距，研究人员开始训练视觉 - 语言模型以获得这些新能力。例如，[11, 15, 17]等研究利用网络屏幕截图数据集来增强大型多模态模型的元素定位能力。同时，[10, 30]等工作通过指令调整模型来解决多步导航任务。

尽管取得了这些进展，为 GUI 视觉智能体训练多模态模型在建模和训练方面仍然面临重大挑战：
- （a）昂贵的视觉建模：UI 屏幕截图通常是高分辨率的（例如 2K），导致标记序列很长，给长上下文处理带来问题。大多数现有模型没有针对此类高分辨率数据进行优化，导致效率低下和高计算成本。
- （b）管理交错的视觉 - 语言 - 动作：动作与语言模态不同，并且可能因设备而异（例如，网络界面上的“返回”与移动设备上的“按主页”），并适应不同的参数设置（例如，“滚动”动作在网络上有两个方向，但在移动平台上有四个方向），如何有效地对动作进行建模尚不清楚。此外，必须将动作与视觉和查询数据一起建模。例如，导航过程会生成屏幕截图和动作步骤的历史记录，创建一个复杂的交错视觉 - 语言 - 动作，模型必须有效地解释和管理。
- （c）多样化的训练数据：在网络和移动等不同设备上存在大量的 GUI 数据，伴随着包括元素定位和导航在内的各种目的注释，目前尚不清楚如何有效地选择高质量的训练语料库来开发强大的 GUI 视觉模型。这些关键挑战尚未得到充分探索，但对于开发有效的 GUI 智能体视觉模型至关重要。

在这项工作中，我们开发了一种用于 GUI 视觉智能体的视觉 - 语言模型，旨在解决和克服上述挑战，主要贡献如下：
- （i）UI 引导的视觉标记选择：我们认识到 UI 屏幕截图的独特性（即冗余与关键细节混合），并开发了一种 UI 友好的视觉标记选择方法。在 RGB 空间中，我们将每个补丁表示为一个节点，并识别连接组件以对补丁之间的冗余进行建模。这种关系指导视觉编码器或语言模型中的自注意力模块进行标记选择，有效地降低计算量。
- （ii）交错的视觉 - 语言 - 动作流：我们分析了 GUI 动作的多样性，以 JSON 格式构建它们并记录其动作空间，以帮助模型进行动作解释。此外，我们认识到跨模态交错理解的必要性，例如将动作与视觉导航历史相结合，并通过多轮动作与文本查询平衡视觉标记长度，以提高训练效率。我们的模型被制定为交错的视觉 - 语言 - 动作流，统一了 GUI 场景中的各种需求。
- （iii）精心挑选的指令跟随数据集：我们没有利用所有可用来源的数据，而是对每种数据类型的属性进行了深入分析。例如，在网络数据中，视觉元素（即按钮）比静态文本（占 40%）更有价值，因为大多数视觉语言模型（VLMs）[41]具有强大的光学字符识别（OCR）能力。此外，我们引入了一个小型、高质量的指令跟随数据集，实现了强大的 UI 定位性能。此外，我们开发了一种再平衡采样策略来解决 UI 数据中的严重不平衡问题，确保在不同设置下模型性能的一致性。

基于上述创新，我们增强了 Qwen2 - VL - 2B 以创建强大的 GUI 视觉智能体 ShowUI。如图 1 所示，这产生了一个使用 256K 数据的轻量级 2B 模型，在零样本屏幕截图定位中达到了 75.1% 的高准确率。ShowUI 在网络[12]、移动[36]和在线[40]环境中也展示出了具有竞争力的导航能力。全面的消融研究（图 1）证明了我们 UI 引导的标记选择方法的有效性，减少了 33% 的冗余视觉标记并加速训练 1.4 倍。此外，我们对当前性能差距和未来方向进行了许多讨论。

## 2. ShowUI
如图 3 所示，ShowUI 建立在视觉 - 语言模型 Qwen2 - VL - 2B[41]的基础上，包含以下针对 GUI 任务优化的关键组件：（i）一种新颖的 UI 引导视觉标记选择策略，用于高效视觉建模；（ii）交错的视觉 - 语言 - 动作流设置，通过 GUI 任务灵活统一不同需求并提高训练效果；（iii）训练数据配方，通过对单个 GUI 数据类型的详细分析精心制作，使 ShowUI 能够在更小的高质量语料库上进行训练。在以下各节中，我们将详细介绍每个组件。

### 2.1 UI 引导的视觉标记选择
高分辨率屏幕截图在标准分割后可能会产生大量视觉标记。如图 4a 所示，在 PC 上分辨率为 1344×756 的图像，以 14×14 进行分割会产生约 5184 个原始标记，经过 2×2[41]合并后仍有 1296 个标记，这在自注意力模块中带来了计算挑战。

图 3. ShowUI 示意图。给定用户任务查询、预定义的动作空间和初始屏幕截图作为观察，ShowUI 通过执行下一个动作、更新屏幕截图并继续此循环来进行。值得注意的是，ShowUI 具有以下关键创新设计：（i）UI 引导的视觉标记选择，它处理输入屏幕截图以构建 UI 补丁连接图。在训练期间，在每个组件内随机选择标记子集以进行高效视觉建模（第 2.1 节）。（ii）交错的视觉 - 语言 - 动作流，以有效处理过去的屏幕截图和动作，提高导航性能。（第 2.2 节）

UI 与自然视觉有何不同？与捕捉现实世界复杂性和不可预测模式、富含语义和纹理的自然图像不同，UI 屏幕截图具有固有的结构，具有清晰的布局和一致的配色方案，经过优化以提高可读性和可用性。这种差异意味着 UI 图像通常包含冗余的空白区域或简单背景，这些区域不携带关键信息，可进行优化或修剪。此外，像图标或文本这样小但功能重要的元素，由于其在交互性和清晰度方面的作用，需要更高的显著性。

因此，有必要制定一种策略，能够区分冗余和关键视觉元素，从而在不影响可用性的情况下有效地修剪无关视觉标记。我们发现 RGB 空间可为此目的提供有用指导，因为模式变体、文本字体可通过其 RGB 值轻松识别。构建 UI 连接图。将屏幕截图划分为规则补丁后，我们观察到许多相邻补丁具有完全相同的 RGB 值，因此是冗余的。为了利用这一点，我们将每个补丁表示为图中的一个节点。如果相邻补丁具有相同的 RGB 值，我们连接它们对应的节点，形成连接组件。这使我们能够对冗余区域进行分组和简化，同时保留由其独特 RGB 模式确定的关键视觉元素。通过在补丁张量的差异上设置一个小阈值，可以轻松检测到视觉上相同的补丁。基于此见解，我们使用并查集方法在该 UI 连接图中识别连接组件，如算法 1 所述。该算法产生一个具有 K 个连接组件的图，其中 K 通常小于原始补丁数量 $G_{h}×G_{w}$。基于每个节点到其组件的分配，我们可以对补丁之间的冗余关系进行建模。

如图 4a 所示，此方法可根据屏幕截图的视觉信息量自适应地平衡组件数量，在稀疏区域较多的谷歌搜索页面中使用较少组件（更多冗余补丁）（1296 → 291），而在文本丰富的 Overleaf 屏幕截图中分配更多组件（更多独立补丁）（1296 → 986）。在图 5 中，我们展示了我们的方法如何在不同设备上构建 UI 连接图。对于具有相同初始视觉补丁标记（例如 1272）的相同分辨率屏幕截图，我们的方法根据屏幕截图的信息量自适应地构建连接组件。

### 算法 1 在 UI 图上查找连接组件
1: 输入：大小为 H×W 的屏幕截图、补丁大小 c、阈值 δ \
2: 输出：补丁与连接组件之间的分配映射。\
3: 将图像划分为 $G_{h}×G_{w}$ 个补丁，每个补丁为一个节点，其中 $G_{h}=H\div c$ 且 $G_{w}=W\div c$\
4: 在节点上初始化并查集结构 UF\
5: 对于所有节点 (i, j) 执行以下操作\
6: &nbsp;&nbsp;&nbsp;&nbsp;对于 (i, j) 右侧和下方的所有邻居 (i′, j′) 执行以下操作\
7: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果 $\left\lVert RGB(i, j)-RGB(i^{\prime}, j^{\prime})\right\rVert<\delta$，则\
8: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UF.union((i, j), (i′, j′))\
9: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结束条件\
10: &nbsp;&nbsp;&nbsp;&nbsp;结束循环\
11: 结束循环\
12: 返回 UF 的分配映射

### 标记合并与标记选择
接下来，我们探索如何利用此 UI 连接图提高模型效率。我们发现现有视觉 - 语言模型的主要计算瓶颈在于级联自注意力层处理的长序列，这对语言模型和视觉编码器都有影响。

一种直接的方法是应用标记合并方法[6, 20]，将组件内的所有补丁表示为单个标记，如图 4b 左半部分所示。然而在实践中，我们发现这种方法会破坏位置关系，因为合并后的标记序列中不可避免地丢失了原始位置信息，而这对于准确的 UI 元素定位至关重要。

为了在自注意力中实现标记压缩而不丢失位置信息，我们从深度混合（Mixture - of - Depth）[35]中获得灵感，它通过路由机制稀疏采样标记。在我们的情况下，UI 连接图提供了有效的路由标准。同一组件内的标记可视为冗余，因此在训练期间我们随机跳过每个组件内的一部分标记，使单补丁组件不受影响以保留关键元素。对于选定的标记，我们保留其原始位置嵌入，确保自注意力在原始位置关系上操作，即使标记序列较短。

值得注意的是，这种标记选择方法不会引入额外的可学习参数。因此，我们在训练期间以设定比例应用随机标记选择，而在推理时，该方法提供了使用或不使用标记选择的灵活性，因为两种选择都能在完整标记序列中保持一致的位置关系。

### 2.2 交错的 VLA 流
在本节中，我们旨在解决如何制定动作及其与其他模态（即视觉和文本查询）的关系。

动作与自然文本有何不同？GUI 模型的核心功能是导航，基于文本查询，要求模型联合预测：正确的动作类型（例如 [CLICK] 或 [TYPE]）以及相应的动作参数（例如 [CLICK] 的坐标或 [TYPE] 的字符串）。导航中的一个主要挑战来自不同设备上的动作变体，例如：
- （i）特定设备的动作（例如 [CLICK] 在移动设备上不可用，而 [PRESS HOME] 在网络上不存在）。
- （ii）具有不同参数的相同动作（例如 [SCROLL] 在网络上有上下两个方向，但在移动设备上有四个方向）。
- （iii）测试时出现的新动作，在训练期间未遇到。

为了在我们的模型中管理这些变化，我们首先将每个动作构建为 JSON 格式（即 {"action":"动作类型", "value":"元素", "position":[x,y]}），其中 [x,y] 表示 0 - 1 范围内的相对坐标。这使我们能够将来自不同设备的动作标准化为统一格式。其次，我们在系统提示中为模型提供一个“README”，记录动作空间中每个动作的用法（例如，“CLICK”：点击一个元素，值不适用，位置 [x,y] 是必需的）。

### 2.3 GUI 指令调整
社区中有各种 GUI 数据集，例如主要的网络数据[10, 11, 44]、移动数据[30, 36]，其中可能包含元素坐标[11]或用户轨迹[30]。我们没有聚合所有可用数据源，而是分析每种数据集类型以选择代表性数据。我们选择的数据如表 1 所示。我们的讨论主要集中在 UI 定位数据上。对于导航，我们从 GUIAct[10]获取移动和网络设备的数据。

（i）网络 - 视觉元素：网络提供了高度可访问、文本丰富的 UI 数据源，可从 HTML[44]轻松抓取。我们的统计分析表明，“静态文本”标签占很大一部分（40%）。鉴于大多数视觉语言模型（VLMs）[41]已经具备强大的光学字符识别（OCR）能力，我们专注于收集视觉丰富的元素。为此，我们开发了一个解析器，并收集了 22K 张屏幕截图，仅保留与视觉相关的元素，如标记为“按钮”或“复选框”的元素，通过移除静态文本来实现。

（ii）桌面 - 多样化查询：桌面数据特别有价值，因为它难以自动收集。我们确定了 OmniAct[22]，它包含来自 iOS、Windows 和 Linux 桌面的手动元素，规模较小（100 张图像中有 2K 个元素）。其元素仅由原始名称标记，如“message_ash”。为了丰富这个数据集并增加多样性，我们采用了逆向工程技术，利用真实边界框及其文本元素。然后，我们使用视觉提示突出目标元素，提示 GPT - 4o 推导出三种类型的查询：外观、空间和意图；如图 7 所示。这种方法产生了 6K 个新元素。详细提示和讨论见补充材料。


图 7. 我们在 GPT - 4o 的协助下，从原始注释中推导出三种类型的查询（外观、空间关系和意图）。

在图 8 中，我们展示了更多关于如何利用 GPT - 4o 根据外观、空间关系和意图，用多样化查询扩充原始 OmniAct - 桌面注释的示例。

（iii）移动 - 功能：移动数据在 Android 中很容易获取，如[25, 36]，它提供图标字幕。值得注意的是，我们认为[8]具有有价值的功能描述，超越了简单的原子元素名称。

### 通过采样平衡数据
如表 1 所示，数据规模在不同类型之间差异显著（例如，只有 100 个桌面样本）。为了确保每种类型都有公平的曝光机会，我们在训练期间开发了一种采样策略，使每个批次包含不同数据类型的概率相当。

## 3. 实验
### 3.1 基准数据集
我们使用以下基准评估我们的模型：
- **定位**：我们使用 Screenspot[11]，这是一个零样本定位评估基准，包含来自三个设备的多样化数据，用于分别评估文本和小部件识别。
- **导航**：我们在来自不同设备的四个不同数据集上评估导航性能：（i）网络上的 Mind2Web[12]，其动作空间包括三种类型的动作。（ii）移动设备上的 AITW[36]，其动作空间包括 11 种动作。（iii）在线的 MiniWob[40]，这是一个在线环境，有两种类型的动作，用于补充离线基准并测试交互设置中的性能。训练设置的详细信息在补充材料中提供。

### 3.2 主要结果
我们针对每个下游任务组织实验，以解决以下问题：
Q1：我们的模型在每个任务上的表现如何？与现有视觉语言模型（VLM）基线相比有哪些改进？
Q2：每个组件的效果和改进是什么？
Q3：基于每个基准的独特属性可以获得哪些见解？

#### 3.2.1 定位任务
在表 2 中，我们展示了在 Screenspot[11]上的零样本定位结果。这为每个设置中的缺点提供了直接信号。我们包括一个额外的变体——ShowUI - G，它仅使用定位数据进行训练。我们的发现包括：（i）在所有方法中，文本轨道得分高于图标轨道，即使对于在训练期间较少出现的桌面文本也是如此。这表明主要从网络和移动设备学习的文本定位能力可跨平台转移。（ii）当使用有效的采样策略时，混合导航数据[10]不会损害定位性能。（iii）由于其视觉定位，图标轨道更具挑战性。移动设备得分显著高于桌面和网络设备，这强调了除移动设备之外缺乏视觉 UI 定位数据。（iv）值得注意的是，ShowUI 作为训练数据最少的最轻量级方法，实现了最先进的定位性能。

#### 3.2.2 导航任务
- **移动：AITW**：在表 3 中，我们有以下发现：（i）没有交错流（即没有视觉历史）时，ShowUI†相对于 VLM 基线仅提供 1.1% 的准确率提升。然而，有视觉历史时，ShowUI 获得了额外的 1.7% 准确率增益，这可能是因为视觉上下文对于适应大动作空间（11）内频繁的软件变化至关重要，特别是在移动平台上。（ii）ShowUI 从 GUIAct[10]学习的零样本导航具有可转移性，这表明可以进一步改进导航组件。（iii）ShowUI 优于 OmniParser[31]和 PaLM2 - CoT[37]，它们要么利用闭源 API，要么使用 HTML 信息，突出了其作为独立视觉智能体的单模型潜力。
- **网站：Mind2Web**：在表 4 中关于网络导航，我们发现：（i）指令调整具有显著效果，相对于 Qwen2 - VL - 2B 带来了 4.6% 的平均步长成功率提升。值得注意的是，ShowUI - 2B 的零样本性能与经过预训练和微调的 SeeClick - 9.6B 相当，并实现了相对较高的操作 F1 值（80% +）。（ii）此任务中的视觉上下文不如在 AITW 中重要，可能是因为 Mind2Web 主要关注单个视觉相似的网站，并且仅包括三个动作。（iii）跨网站和跨域设置比跨任务更困难。这表明瓶颈在于 UI 视觉感知而非文本任务理解（网站/域在训练中未出现）。一个未来的改进方向是开发具有良好（视觉）域多样性的训练数据。
- **在线：MiniWob**：在表 5 中，这个基准展示了模型在在线环境中的行为。我们的关键发现是，尽管 MiniWob UI 很简单，但 ShowUI 的零样本性能（27.1%）与微调后的 Qwen - VL（48.4%）之间存在很大差距。相比之下，ShowUI 的零样本性能在 Mind2Web 中超过了其他模型，这可能是因为在指令调整阶段未充分解决分布外误差。这表明仅离线指令调整是不够的；我们需要开发一种针对在线环境的学习策略，以处理新的错误情况。

### 3.3 消融研究
- **UI 引导标记选择的影响**：在表 9a 中，我们通过以下变体检查各种视觉标记优化方法：（i）基线：未应用视觉标记优化策略；（ii）标记合并：第 2.1 节中介绍的主流方法，基于我们的 UI 图；（iii）标记选择 - 随机：随机选择视觉标记子集的变体，作为直接基线；（iv）标记选择 - UI 图：我们提出的利用 UI 图进行标记选择的方法。

如所示，标记合并的表现比随机选择差，突出了保留标记之间位置关系的重要性。标记选择 - UI 图提供了良好的平衡，加速 1.5 倍且具有竞争力的准确率。虽然在测试时应用它会因分辨率损失而略微降低准确率，但它仍然比随机选择更可靠，证明了 UI 连接图作为指导标准的有效性。
- **层插入的选择**：在表 9b 中，我们对不同插入策略进行消融研究，包括在所有层插入、早期或晚期插入 X 层以及交叉层插入，其中层在插入和未插入之间交替。在插入层数相同的情况下，交叉层插入明显优于早期和晚期插入。
- **不同选择比例**：结果如表 9c 所示，说明了选择比例是加速和性能之间的权衡，0.5 是一个有效的选择。
- **交错流的影响**：我们使用交错流对定位和导航任务进行迭代性能评估，以研究其影响。（i）动作 - 查询：在图 10 中，我们比较了有无多轮流的定位训练。多轮流导致更快的进展，特别是在初始预热阶段，并保持性能差距，证明了数据利用率的提高。（ii）动作 - 视觉：如前面带有 ShowUI†变体的表格所示，我们验证了视觉上下文的影响。图 11 展示了模型在迭代步骤中的进展，趋势表明视觉 + 动作 + 多轮优于视觉 - 动作和仅动作设置。这验证了我们的交错流是一种有效且高效的策略。
- **指令调整数据的影响**：我们的贡献之一是在第 2.3 节中对定位任务的训练数据进行分析。在表 6 中，我们进行了详细的消融研究，以研究每个变化对特定设备和设置的单独影响。

我们发现（i）数据质量很重要：仅有 2K 个元素的 OmniAct 与网络数据得分相当，并且当用 GPT - 4o 扩充为多样化查询时，它增强了模型的泛化能力并优化了数据使用效率。（ii）我们收集的 22K 网络数据优于 SeeClick 的 270K 屏幕截图；此外，通过视觉标准过滤网络数据可显著减少元素大小而不影响性能，这表明静态文本信息量较少——这是 VLM 固有的属性。（iii）平衡采样至关重要，可带来 3.7% 的准确率提升，并在各个设置中保持性能。

### 3.4 定性示例
在图 12 和 13 中，我们展示了 Screenspot 零样本定位的几个示例。我们发现：通过指令调整，ShowUI 能够进行一些视觉推理，例如它可以在多个抽象符号中区分正确的操作符，或者将“查看帮助”与问号相关联，如图 12（b，e）所示。此外，在一些失败案例中，如图 12（d，f）所示，可能存在多个可能的可点击元素，导致模型混淆。

## 4. 相关工作
### GUI 智能体
最近的研究揭示了大语言模型（LLMs）在语言建模之外的潜力，[14, 43, 48, 49, 51]等方面的进展表明它们能够使用工具集成自主完成复杂任务。这促使了 GUI 自动化方法的发展，例如：（i）免训练系统，其分阶段运行：首先，它们通过将 GUI 转换为 HTML 表示、访问辅助功能树或使用光学字符识别（OCR）[24]和 SoM[46, 47, 55]等视觉模型收集 UI 信息。接下来，LLMs 整合此信息以生成响应。这种方法严重依赖闭源 LLMs[32]，导致成本高昂且应用有限，因为现实世界用户通常只感知屏幕截图而非这些权威数据源。为了解决这些限制，（ii）基于训练的模型[11, 17, 27, 50]被提出，这些模型在大规模 UI 视觉 - 文本语料库（例如屏幕截图）上进行预训练，缩小视觉感知差距以实现元素定位或导航等能力。我们的工作属于第二类，专注于开发 GUI 视觉智能体的关键挑战，例如高效建模高分辨率视觉屏幕截图以及利用交错流进行历史管理。

### 视觉 - 语言 - 动作模型
视觉 - 语言模型最近取得了重大进展，能够处理视觉和文本数据，有效地作为灵活的聊天机器人。然而，当需要与现实世界环境交互时，它们仍然面临局限性。为了解决这个问题，视觉 - 语言 - 动作（VLA）模型被开发出来，以增强对物理或数字环境的感知和动作生成。著名的例子包括 RT - 2 - X[33]和 OpenVLA[23]，它们使 VLMs 能够在自然环境中执行动作，如机器人技术[7, 18, 54]、自动驾驶[3]和游戏[28, 42]。虽然在具身环境中取得了很大进展，但 VLA 模型在数字 GUI 中的集成仍未得到充分探索。在我们的工作中，我们为数字 GUI 环境开发视觉 - 语言 - 动作（VLA）模型以填补这一差距，专注于 GUIs 的独特属性，并解决如何在我们的 VLA 框架内与其他模态一起对动作进行建模。

### 高效视觉表示
大型多模态模型的计算成本是一个重大瓶颈，特别是在使用高分辨率屏幕截图扩展序列长度时。减少训练成本的直接方法，如标记修剪[9, 38]和标记合并[21, 26]，不适合 GUI 场景，因为 GUIs 需要细粒度的位置信息而非高级语义。这些技术丢弃了关键的空间细节，从而损害了元素定位。作为替代方案，深度混合（MoD）[35, 45]提供了一种有前途的方法，通过在模型深度上分配计算来处理语言标记，平衡性能与速度，并保留单个标记的位置关系。

在我们的工作中，我们开发了一种 UI 友好的高效视觉标记选择解决方案。高分辨率 UI 屏幕截图通常包含冗余区域，如过多的空白空间，同时保持结构化布局。为了利用这一点，我们在 RGB 空间中通过将每个补丁视为一个节点并找到连接组件来构建 UI 连接图，以捕捉冗余关系。此图指导自注意力模块跳过部分冗余标记，在不引入额外可学习参数的情况下降低计算成本。

## 5. 结论
我们引入了 ShowUI，这是一种用于 GUI 视觉智能体的视觉 - 语言 - 动作模型，解决了 UI 视觉和动作建模以及指令调整数据整理中的关键挑战。从模型方面来看，我们开发的 UI 引导视觉标记选择允许高效处理高分辨率 UI 屏幕截图，显著降低计算成本。我们的交错视觉 - 语言 - 动作流框架有效地管理了跨模态的复杂交互。从数据方面来看，通过精心整理的高质量指令跟随数据集，ShowUI 以轻量级模型规模实现了强大的性能。这些结果展示了 ShowUI 在推动 GUI 视觉智能体向更类人交互和感知发展方面的潜力。


### 参考资料

- 标题：ShowUI: One Vision-Language-Action Model for GUI Visual Agent
- 作者：Kevin Qinghong Lin¹, Linjie Li², Difei Gao¹, Zhengyuan Yang², Shiwei Wu¹, Zechen Bai¹, Stan Weixian Lei¹, Lijuan Wang², Mike Zheng Shou¹
- 单位：1. Show Lab, National University of Singapore；2. Microsoft
- 标签：视觉语言动作模型、GUI 智能体、视觉标记选择、交错流、指令跟随数据集
- 概述：本文介绍了用于 GUI 视觉智能体的 ShowUI 模型，通过 UI 引导的视觉标记选择、交错的视觉 - 语言 - 动作流和精心挑选的指令跟随数据集等创新，以轻量级模型和少量数据实现高准确率定位及有效导航，提升 GUI 智能体性能。
- 链接：https://arxiv.org/pdf/2411.17465
