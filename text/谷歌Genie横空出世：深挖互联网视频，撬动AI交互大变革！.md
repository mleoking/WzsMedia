# 谷歌Genie横空出世：深挖互联网视频，撬动AI交互大变革！

谷歌Genie模型的出现标志着人工智能在生成交互式虚拟环境方面迈出了重要一步，它能够从海量的互联网视频数据中学习并创建出全新的、可控制的虚拟世界，这为游戏开发、虚拟现实和自动化训练等领域带来了新的可能性。

## 摘要
我们引入Genie，这是首个以无监督方式从无标记互联网视频中训练出来的生成式交互环境。该模型可以根据文本、合成图像、照片甚至草图等提示，生成各种各样可通过动作控制的虚拟世界。Genie拥有110亿参数，可以被视为一个基础世界模型。它由一个时空视频分词器、一个自回归动态模型和一个简单且可扩展的潜在动作模型组成。尽管Genie在训练时没有任何真实动作标签或世界模型文献中常见的其他特定领域要求，但它使用户能够在生成的环境中逐帧进行操作。此外，所得到的学习到的潜在动作空间有助于训练智能体模仿来自未见过视频中的行为，为训练未来的通用智能体开辟了道路。
## 1. 引言
在过去几年中，生成式人工智能兴起，模型能够生成新颖且有创意的内容。在Transformer（Vaswani等人，2017）等架构的突破、硬件的进步以及近期对模型和数据集扩展的关注推动下，我们现在能够从文本提示生成连贯的对话语言（Brown等人，2020；Radford等人，2018，2019），以及清晰且美观的图像（Ramesh等人，2021，2022；Rombach等人，2022；Saharia等人，2022）。早期迹象表明视频生成将是另一个前沿领域，近期结果表明此类模型也可能受益于规模（Blattmann等人，2023a；Esser等人，2023；Ho等人，2022a；Hong等人，2023）。尽管如此，视频生成模型与ChatGPT等语言工具在交互和参与程度上仍存在差距，更不用说更具沉浸感的体验了。

如果给定大量来自互联网的视频，我们不仅能训练出能够生成新颖图像或视频的模型，还能生成整个交互体验，那会怎样呢？我们提出生成式交互环境，这是生成式人工智能的一种新范式，通过该范式可以从单个文本或图像提示生成交互环境。我们的方法Genie，是从超过20万小时的公开可用互联网游戏视频的大型数据集中训练出来的，并且尽管在训练时没有动作或文本注释，但通过学习到的潜在动作空间可以逐帧进行控制（见表1与其他方法的比较）。Genie拥有110亿参数，展现出基础模型中常见的特性——它可以将未见过的图像作为提示，从而能够创建和玩完全想象出来的虚拟世界（例如图2）。

Genie基于最先进的视频生成模型（Gupta等人，2023；Villegas等人，2023）的思想构建，核心设计选择是在我们所有模型组件中使用时空（ST）Transformer（Xu等人，2020）。Genie利用一种新颖的视频分词器，并通过因果动作模型提取潜在动作。视频标记和潜在动作都被传递到动态模型中，该模型使用MaskGIT（Chang等人，2022）自回归地预测下一帧。我们对架构在批量和模型大小方面进行了严格的扩展分析，将参数从4000万变化到27亿。结果表明，我们的架构随着计算资源的增加而优雅地扩展，最终得到了一个110亿参数的模型。我们在从数百个2D平台游戏中筛选出的3万小时互联网游戏视频集上训练Genie，为该设置生成了一个基础世界模型。

为了证明我们方法的通用性，我们还在来自RT1数据集（Brohan等人，2023）的无动作机器人视频上训练了一个单独的模型，学习到了具有一致潜在动作的生成环境。最后，我们表明从互联网视频中学习到的潜在动作可用于从未见过的模拟强化学习（RL）环境的无动作视频中推断策略，这表明Genie可能是为训练下一代通用智能体解锁无限数据的关键（Bauer等人，2023；Clune，2019；Open Ended Learning Team等人，2021；Reed等人，2022）。

与传统Transformer中每个标记都关注所有其他标记不同，ST - Transformer包含𝐿个时空块，其中交织着空间和时间注意力层，随后是一个标准注意力块中的前馈层（FFW）。空间层中的自注意力关注每个时间步内的 $1×H×W $个标记，而时间层中的自注意力关注跨T个时间步的 $T×1×1 $个标记。与序列Transformer类似，时间层采用因果结构并带有因果掩码。关键是，我们架构中计算复杂度的主导因素（即空间注意力层）与帧数成线性比例，而不是二次方，这使得在具有一致动态的扩展交互中进行视频生成时效率更高。此外，请注意在ST块中，我们在空间和时间组件之后仅包含一个FFW，省略了空间后的FFW，以便能够扩展模型的其他组件，我们观察到这显著改善了结果。
## 2. 方法
Genie是一个从仅视频数据训练而来的生成式交互环境。在本节中，我们先进行预备知识介绍，然后解释模型的主要组件。

Genie架构中的几个组件基于视觉Transformer（ViT）（Dosovitskiy等人，2021；Vaswani等人，2017）。值得注意的是，Transformer的二次方内存成本给视频带来了挑战，因为视频可能包含多达 $O(10^{4}) $个标记。因此，我们在所有模型组件中采用内存高效的ST - Transformer架构（受Xu等人（2020）启发，见图4），在模型容量和计算约束之间取得平衡。
### 2.1 模型组件
如图3所示，我们的模型包含三个关键组件：1）一个潜在动作模型，用于推断每对帧之间的潜在动作𝒂；2）一个视频分词器，将原始视频帧转换为离散标记𝒛；3）一个动态模型，给定潜在动作和过去的帧标记，预测视频的下一帧。模型按照标准的自回归视频生成管道分两个阶段进行训练：我们首先训练视频分词器，然后将其用于动态模型。然后我们联合训练潜在动作模型（直接从像素）和动态模型（在视频标记上）。

|模型类别|训练数据|可控性|
|---|---|---|
|世界模型|视频 + 动作|帧级|
|视频模型|视频 + 文本|视频级|
|Genie|视频|帧级|

### 潜在动作模型（LAM）
为了实现可控的视频生成，我们将每个未来帧的预测条件设定为在前一帧采取的动作。然而，在互联网视频中这样的动作标签很少见，并且获取动作注释可能成本高昂。相反，我们以完全无监督的方式学习潜在动作（见图5）。

首先，一个编码器将所有先前帧 $x_{1:t}=(x_{1},\cdots x_{t}) $以及下一帧 $x_{t + 1} $作为输入，并输出相应的一组连续潜在动作 $\tilde{a}_{1:t}=(\tilde{a}_{1},\cdots \tilde{a}_{t}) $。然后一个解码器将所有先前帧和潜在动作作为输入，并预测下一帧 $\hat{x}_{t + 1} $。

为了训练模型，我们利用基于VQ - VAE的目标（van den Oord等人，2017），这使我们能够将预测动作的数量限制为一小部分离散代码。我们将VQ码本的词汇大小 $[A] $（即可能的潜在动作的最大数量）限制为一个较小的值，以允许人类可玩性并进一步加强可控性（在我们的实验中使用 $|A| = 8 $）。由于解码器只能访问历史和潜在动作， $\tilde{a}_{t} $应该编码过去和未来之间最有意义的变化，以便解码器成功重建未来帧。请注意，这个解码器仅用于给LAM训练信号。实际上，除了VQ码本，整个LAM在推理时被丢弃，并被用户的动作所取代。

我们将ST - Transformer架构用于潜在动作模型。时间层中的因果掩码允许我们将整个视频 $x_{1:T} $作为输入，并生成每对帧之间的所有潜在动作 $\tilde{a}_{1:T - 1} $。

### 视频分词器
遵循先前的工作（Gupta等人，2023；Villegas等人，2023；Yan等人，2023），我们将视频压缩为离散标记以降低维度并实现更高质量的视频生成（见图6）。我们再次使用VQ - VAE，它将T帧视频 $x_{1:T}=(x_{1},x_{2},\cdots,x_{T})\in \mathbb{R}^{T×H×W×C} $作为输入，为每一帧生成离散表示 $z_{1:T}=(z_{1},z_{2},\cdots,z_{T})\in \square^{T×D} $，其中D是离散潜在空间的大小。分词器使用整个视频序列上的标准VQ - VAE目标进行训练。

与先前在分词阶段仅关注空间压缩的工作（Gupta等人，2023；Hong等人，2022；Wu等人，2022）不同，我们在编码器和解码器中都使用ST - Transformer，将时间动态纳入编码中，这提高了视频生成质量。由于ST - Transformer的因果性质，每个离散编码 $z_{t} $包含来自视频中所有先前看到的帧 $x_{1:t} $的信息。Phenaki（Villegas等人，2023）也使用了一个时间感知分词器C - ViViT，但这种架构计算密集，因为成本随着帧数呈二次方增长——相比之下，我们基于ST - Transformer的分词器（STViViT）计算效率更高，其成本的主导因素随着帧数线性增长。

### 动态模型
动态模型是一个仅解码器的MaskGIT（Chang等人，2022）Transformer（见图7）。在每个时间步 $t\in[1,T] $，它将标记化的视频 $z_{1:t - 1} $和停止梯度的潜在动作 $\tilde{a}_{1:t - 1} $作为输入，并预测下一帧标记 $\hat{z}_{t} $。我们再次使用ST - Transformer，其因果结构使我们能够使用来自所有 $(T - 1) $帧 $z_{1:T - 1} $和潜在动作 $\tilde{a}_{1:T - 1} $作为输入，并生成对所有下一帧 $\hat{z}_{2:T} $的预测。模型使用预测标记 $\hat{z}_{2:T} $和真实标记 $z_{2:T} $之间的交叉熵损失进行训练。在训练时，我们根据在0.5到1之间均匀采样的伯努利分布掩码率随机掩码输入标记 $z_{2:T - 1} $。请注意，训练世界模型（包括基于Transformer的模型）的一种常见做法是将时间𝑡的动作连接到相应的帧（Micheli等人，2023；Robine等人，2023）。然而，我们发现将潜在动作视为潜在动作模型和动态模型的加法嵌入有助于提高生成的可控性。
### 2.2 推理：动作可控视频生成
我们现在描述如何在推理时使用Genie进行动作可控视频生成（见图8）。玩家首先用一个图像 $x_{1} $提示模型，该图像作为初始帧。图像使用视频编码器进行分词，产生 $z_{1} $。玩家然后通过选择 $[0,|A|) $内的任何整数值指定一个离散潜在动作 $a_{1} $。动态模型将帧标记 $z_{1} $和相应的潜在动作 $\tilde{a}_{1} $（通过用离散输入 $a_{1} $索引VQ码本获得）作为输入，预测下一帧标记 $z_{2} $。这个过程重复进行，以自回归方式生成序列的其余部分 $\hat{z}_{2:T} $，同时动作继续传递给模型，而标记通过分词器的解码器解码为视频帧 $\hat{x}_{2:T} $。请注意，我们可以通过将起始帧和从数据集中推断出的动作传递给模型来重新生成真实视频，或者通过改变动作生成全新的视频（或轨迹）。
## 3. 实验结果
### 数据集
我们在从公开可用的2D平台游戏互联网视频中收集的大规模数据集上训练Genie（此后称为“平台游戏”）。我们通过过滤与平台游戏相关的关键字的公开可用视频来构建平台游戏数据集，得到5500万个16秒的视频片段，帧率为10FPS，分辨率为160x90。最终数据集包含680万个16秒的视频片段（3万小时），与其他流行的互联网视频数据集（Bain等人，2021；Wang等人，2023）在数量级上相当。更多详细信息可以在附录B.1中找到。除非另有说明，结果是使用110亿参数模型在该数据集上训练得到的。

为了验证我们方法的通用性，我们还考虑了用于训练RT1的机器人数据集（Brohan等人，2023），将他们的约13万个机器人演示数据集与一个单独的模拟数据集以及先前工作（Kalashnikov等人，2018）中的20.9万个真实机器人数据片段相结合。请注意，我们不使用这些数据集中的任何动作，只是将它们视为视频。为了简单起见，从这里开始我们将这个数据集称为“机器人学”。
### 指标
我们通过两个因素检查Genie的视频生成性能，即视频保真度（即视频生成的质量）和可控性（即潜在动作在视频生成中的影响程度）。对于视频保真度，我们使用弗雷歇视频距离（FVD），这是一种视频级指标，已被证明与人类对视频质量的评估高度一致（Unterthiner等人，2019）。对于可控性，我们设计了一个基于峰值信噪比（PSNR）的指标，我们称之为 $\Delta $PSNR，它测量当基于从真实帧推断的潜在动作（ $\hat{x}_{t} $）与从随机分布采样的潜在动作（ $\hat{x}_{t}' $）生成视频时，视频生成的差异。

 $\Delta_{t}PSNR = PSNR(x_{t},\hat{x}_{t}) - PSNR(x_{t},\hat{x}_{t}') $，
 
 其中 $x_{t} $表示时间t的真实帧， $\hat{x}_{t} $表示从真实帧推断的潜在动作 $\tilde{a}_{1:t} $生成的帧， $\hat{x}_{t}' $表示从分类分布中随机采样的潜在动作序列生成的相同帧。因此， $\Delta_{t} $PSNR越大，从随机潜在动作生成的视频与真实帧的差异就越大，这表明潜在动作的可控性越高。对于所有实验，我们报告 $t = 4 $时的 $\Delta_{t} $PSNR。
### 训练细节
我们的视频分词器使用2亿参数，补丁大小为4，码本的嵌入大小为32，有1024个唯一代码，我们发现这在分词器的重建质量和视频预测的下游性能之间的权衡中是最有效的。潜在动作模型有3亿参数，补丁大小为16，码本的嵌入大小为32，有8个潜在动作。对于所有建模组件，我们使用16帧的序列长度，帧率为10。此外，我们在训练动态模型时使用bfloat16和QK范数，这已被证明可以在大规模训练中稳定训练（Dehghani等人，2023；Henry等人，2020）。在推理时，我们使用随机采样对每个帧进行25次MaskGIT步骤，温度为2。更多详细信息见附录C。
### 3.1 缩放结果
在本节中，我们研究模型的缩放行为。为此，我们进行了探索模型大小和批量大小影响的研究。有关架构和计算使用的更多详细信息，请参阅附录D。

### 缩放模型大小
给定固定的视频分词器和动作模型架构，我们训练了一系列从4000万到27亿参数的动态模型。图9显示我们的架构随着模型参数的增加而优雅地缩放，每次增加大小都对应着最终训练损失的持续降低。这有力地表明我们的方法受益于缩放，我们在主要的Genie模型中利用了这一点。

### 缩放批量大小
我们还研究了缩放批量大小的影响，考虑了一个23亿参数的模型，批量大小为128、256和448，相当于190万、380万和660万个标记。如图9所示，增加批量大小在模型性能方面也带来了类似的有利提升。

Genie模型显然，增加模型大小和批量大小都有助于提高模型性能。因此，对于我们的最终模型，我们使用批量大小为512训练一个101亿参数的动态模型，共进行12.5万步，使用256个TPUv5p。当与分词器和动作模型结合时，总参数达到107亿，在9420亿个标记上进行训练，我们将其称为Genie模型。对于我们的网站，我们训练了一个更大的解码器，将标记映射到360p视频，增加了额外的参数。
### 3.2 定性结果
我们现在展示Genie模型的定性结果。我们展示了一个在平台游戏数据集上训练的110亿参数模型和一个在机器人数据集上训练的较小模型。我们的模型在不同领域生成高质量、可控的视频。值得注意的是，我们仅使用分布外（OOD）图像提示定性评估在平台游戏数据集上训练的模型，包括从文本到图像模型生成的图像、手绘草图甚至真实照片。能够推广到如此显著的OOD输入突显了我们方法的稳健性以及在大规模数据上训练的价值，而如果以真实动作作为输入，这是不可行的。

Genie模型很明显，增加模型大小和批量大小有助于提高模型性能。因此，对于我们的最终模型，我们使用批量大小为512训练一个101亿参数的动态模型，共进行12.5万步，使用256个TPUv5p。当与分词器和动作模型结合时，总参数达到107亿，在9420亿个标记上进行训练，我们将其称为Genie模型。对于我们的网站，我们训练了一个更大的解码器，将标记映射到360p视频，增加了额外的参数。

平台游戏训练模型图10展示了我们的模型从OOD图像生成的示例，包括（顶行）从Imagen2生成的图像（Ho等人，2022a；van den Oord等人）、（第二行）手绘草图和（底行）真实世界照片。Genie能够将这些想象中的世界变为现实，因为我们在与每个示例交互时看到了类似游戏的行为。我们在附录A中展示了更多模型生成的内容，另外还突出了潜在动作的一致性。

我们模型的另一个新兴能力是它能够理解3D场景并模拟视差，这在平台游戏中很常见。在图12中，我们展示了一张由Imagen2生成的图像，其中采取一个潜在动作时，前景的移动速度与背景不同（如不同颜色箭头的长度所示）。

机器人学训练模型我们使用在平台游戏中找到的最佳超参数在机器人数据集上训练了一个25亿参数的模型，在测试集上实现了82.7的FVD。如图13所示，这个模型成功地从视频数据中学习到了不同且一致的动作，既不需要文本也不需要动作标签（如Yang等人（2023））。值得注意的是，我们的模型不仅学习了机器人手臂的控制，还学习了各种物体的交互和变形（图11）。我们相信这表明我们的方法为使用来自互联网的更大视频数据集创建机器人学的基础世界模型提供了一条途径，具有可用于各种应用的低级可控模拟。

### 3.3 训练智能体
我们相信Genie有一天可以用作训练通用智能体的基础世界模型。在图14中，我们表明该模型已经可以用于在给定起始帧的情况下在未见过的RL环境中生成多样化的轨迹。我们进一步研究从互联网视频中学习到的潜在动作是否可以用于模仿未见过的视频中的行为。我们使用一个冻结的LAM用离散潜在动作标记来自目标环境的一系列专家视频，然后训练一个策略，该策略根据观察预测专家采取潜在动作的可能性。然后我们使用一个带有专家真实动作的小数据集将潜在动作映射到真实动作（更多详细信息见附录E）。

我们在程序生成的2D平台环境CoinRun（Cobbe等人，2020）的困难和容易设置中进行评估，并与一个可以访问专家动作作为上限的预言行为克隆（BC）模型和一个随机智能体作为下限进行比较（图15）。基于LAM的策略在仅使用200个专家样本进行适应时就达到了与预言相同的分数，尽管几乎可以肯定它以前从未见过CoinRun。这提供了证据表明学习到的潜在动作是一致的且对于迁移是有意义的，因为从潜在动作到真实动作的映射不包含关于当前观察的信息。
### 3.4 消融研究
### 潜在动作模型的设计选择
在设计我们的潜在动作模型时，我们仔细考虑了使用的输入类型。虽然我们最终选择使用原始图像（像素），但我们将这个选择与使用标记化图像（在图5中用z替换x）的替代方案进行了评估。我们将这种替代方法称为“标记输入”模型（见表2）。

虽然这个模型在平台游戏数据集上实现了略低的FVD分数，但在机器人数据集上并没有保持这种优势。更重要的是，在这两种环境中，标记输入模型表现出更差的可控性（如通过 $\Delta $PSNR测量）。这表明在标记化过程中可能丢失了一些关于视频动态和运动的信息，因此潜在动作模型使用原始视频作为输入是有益的。

| |数据集|#参数|FVD(1)| $\Delta $PSNR(↑)|
|---|---|---|---|---|
|标记输入|平台游戏|23亿|38.8|1.33|
|像素输入（Genie）|平台游戏|25亿|40.1|1.91|
|标记输入|机器人学|10亿|257.8|1.65|
|像素输入（Genie）|机器人学|10亿|136.4|2.07|

### 分词器架构消融
我们比较了三种分词器选择的性能，包括1）（仅空间）ViT、2）（时空）ST - ViViT和3）（时空）C - ViViT（表3）。为了进行比较，我们对所有分词器使用相似数量的参数，补丁大小为10，批量大小为128，序列长度为16。然后我们在这三个不同的分词器上训练相同的动态和潜在动作模型，并报告它们的FVD以及 $\Delta_{t} $PSNR。

| |#参数|内存|FVD(1)| $\Delta $PSNR(↑)|
|---|---|---|---|---|
|ViT|2.3亿|0.3GB|114.5|1.39|
|C - ViViT（Villegas等人，2023）|2.25亿|1.6GB|272.7|1.37|
|ST - ViViT（我们的）|2.05亿|0.9GB|81.4|1.66|

我们提出的ST - ViViT架构在合理的内存权衡下，分别提供了改进的视频生成（FVD）和 $\Delta_{t} $PSNR，与C - ViViT和仅空间的ViT相比。这展示了它分别生成高保真和可控视频的能力。虽然C - ViViT采用了全时空注意力机制，导致在相同参数数量下与其他两种架构相比内存消耗显著更高，但这并没有转化为性能的提高。事实上，C - ViViT表现出过度拟合的趋势，在训练期间需要强正则化，这可能解释了其相当低的性能。
## 4. 相关工作
### 世界模型
生成式交互环境可以被视为世界模型（Ha和Schmidhuber，2018；Oh等人，2015）的一类，它能够根据动作输入进行下一帧预测（Bamford和Lucas，2020；Chiappa等人，2017；Eslami等人，2018；Hafner等人，2020，2021；Kim等人，2020，2021；Micheli等人，2023；Nunes等人，2020；Pan等人，2022；Robine等人，2023）。这样的模型对于训练智能体很有用，因为它们可以用于在智能体训练时无需直接环境经验来学习策略。然而，学习这些模型本身通常需要直接从环境中获取动作条件数据。相比之下，我们的方法旨在仅从视频中以无监督的方式学习世界模型。最近，对扩展世界模型有了新的重视。GAIA - 1（Hu等人，2023）和UniSim（Yang等人，2023）分别学习用于自动驾驶和机器人操作的世界模型。这些方法需要文本和动作标签，而我们专注于从公开可用的互联网视频中的仅视频数据进行训练。
### 环境生成
我们的工作也与程序内容生成（PCG，例如Risi和Togelius，2020a，b）相关，其中机器学习已被证明在生成游戏关卡方面非常有效（Summerville等人，2018），最近通过直接编写游戏代码的语言模型（Sudhakaran等人，2023；Todd等人，2023）。语言模型本身也可以被视为交互环境（Wong等人，2023），尽管缺乏视觉组件。相比之下，在我们的设置中，关卡可以直接从像素中学习和生成，这使我们能够利用互联网视频数据的多样性。
### 用潜在动作训练智能体
先前的工作已经使用潜在动作进行从观察中模仿（Edwards等人，2019）、规划（Rybkin*等人，2019）和预训练RL智能体（Schmidt和Jiang，2024；Ye等人，2022）。这些方法与我们的潜在动作模型有相似的目标，尽管尚未大规模应用。VPT（Baker等人，2022）是一种最近的方法，它使用从人类提供的动作标记数据中学习的逆动力学模型，用动作标记互联网规模的视频，然后可以用于训练策略。我们表明，相比之下，我们可以使用从互联网视频中学习到的潜在动作来推断任意环境的策略，避免了需要昂贵且可能无法推广的真实动作。
## 5. 结论与未来工作
我们提出了Genie，这是一种新形式的生成式人工智能，使任何人，甚至儿童，都能够像在人类设计的模拟环境中一样，构思、创建并进入生成的世界。尽管仅从视频数据进行训练，Genie可以根据提示生成各种各样的交互和可控环境。

可复现性：我们理解对于计算资源较少的研究人员来说，重现我们的主要结果可能具有挑战性。为了缓解这个问题，我们在附录F中描述了一个较小规模、完全可复现的示例，该示例可以在单个中端TPU（或GPU）上运行。鉴于许多设计选择在两种设置之间可转换，我们相信这将使更广泛的社区能够研究未来的架构改进以及我们工作产生的其他研究方向。

模型有明显的改进空间。Genie继承了其他自回归Transformer模型的一些弱点，并且可能产生不现实的未来预测。虽然我们在时空表示方面取得了进展，但我们仍然限于16帧的记忆，这使得在长时间范围内获得一致的环境具有挑战性。最后，Genie目前的运行速度约为1FPS，需要未来的进展来实现有效的交互帧率。

尽管如此，我们相信Genie为未来的研究开辟了巨大的潜力。鉴于其通用性，该模型可以从更大比例的互联网视频中进行训练，以模拟多样化、现实和想象的环境。此外，我们仅简要提及了使用Genie训练智能体的能力，但鉴于缺乏丰富多样的环境是RL中的关键限制之一，我们可以开辟创建更通用智能体的新途径。

### 参考资料
- 标题：Genie: Generative Interactive Environments
- 作者：Jake Bruce, Michael Dennis, Ashley Edwards等
- 单位：Google DeepMind
- 标签：生成式人工智能、基础模型、世界模型、视频模型、无监督学习、强化学习
- 概述：Google DeepMind提出Genie，首个从无标记互联网视频无监督训练的生成式交互环境模型，介绍其模型组件、推理过程、实验结果等，展现其在生成可控虚拟世界等方面的能力与潜力。
- 链接：https://arxiv.org/pdf/2402.15391
- 摘要: 本文介绍了Genie，一个由互联网视频训练而成的无监督基础世界模型，能够根据文本提示生成可控制的虚拟世界。Genie包含一个时空视频分词器、一个自回归动力学模型和一个潜在动作模型，能够在没有地面真实动作标签的情况下，逐帧生成交互式环境。
- 研究背景和重要意义: Genie模型的出现标志着人工智能在生成交互式虚拟环境方面迈出了重要一步，它能够从海量的互联网视频数据中学习并创建出全新的、可控制的虚拟世界，这为游戏开发、虚拟现实和自动化训练等领域带来了新的可能性。
