# NeurIPS2024-FIND：基础模型表征新突破，开启多模态交错任务新征程

 随着基础模型在视觉和语言领域发展，虽能力强大但输出类型受限。本文旨在突破此局限，利用基础模型嵌入扩展输出空间，通过提出FIND接口及相关基准，实现交错理解与推理，推动多模态任务发展，如在机器人导航、3D特征场创建等方面具有潜在应用价值。

## 摘要
基础模型在跨模态推理和记忆方面具有强大能力。为了进一步释放基础模型的潜力，我们提出了FIND，这是一种通用接口，用于将基础模型的嵌入与跨模态和跨粒度的统一图像及数据集级理解进行对齐。如图1所示，一个无需调整任何基础模型权重的轻量级变换器接口，足以以交错方式进行分割、定位和检索。所提出的接口具有以下有利属性：（1）通用性。它适用于各种任务，包括检索、分割等，且在相同架构和权重下。（2）可交错性。得益于多任务多模态训练，所提出的接口创建了一个交错共享嵌入空间。（3）可扩展性。所提出的接口能够适应新任务和新模型。基于交错嵌入空间，我们引入了FIND-Bench，它为COCO数据集引入了新的训练和评估注释，用于交错分割和检索。我们是首个将基础模型的嵌入进行对齐以实现交错理解的工作。同时，我们的方法在FIND-Bench上达到了最先进的性能，在标准检索和分割设置下也具有竞争力。

## 1. 引言
随着基础模型在视觉和语言领域取得令人振奋的进展，例如GPT-4(V) [30]、DALLE-3 [31]、SAM [19]和LLaMA [38]等，我们已经达到了深度学习模型在视觉和语言领域都能实现卓越性能的阶段[5, 22]。具体而言，像GPT-4(V) [30]这样的模型已经展示出了人类水平的感知和推理技能[46]。

尽管这些模型在信息记忆、处理和推理方面具有令人印象深刻的能力，但它们往往针对特定的输出类型进行了专门设计。然而，它们的输出类型有限，例如GPT的输出为语言，DALLE的输出为图像，SAM的输出为掩码等。在这项工作中，我们旨在利用基础模型嵌入的特权属性来扩展其输出空间（例如，扩展到像素级输出），释放它们在交错理解和推理方面的潜力。

为了实现这一目标，我们引入了一个用于基础模型嵌入的接口（FIND），它利用预训练的基础模型嵌入，以交错方式联合处理不同粒度（从像素到图像）的下游任务。如图2.1所示，FIND接口处理来自视觉和语言基础模型的嵌入，并输出分割、定位和检索结果。

由于在FIND中所有视觉 - 语言任务都进行了统一训练，因此创建了一个交错共享嵌入空间，其中视觉和语言参考可以相互交换和增强。例如，在图2.2中，在映射期间，交错表示放松了源域和目标域上的单模态约束。在推理期间，与多模态序列相比，交错序列增强了视觉和语言之间的信息交换。

为了有效地对齐和评估交错嵌入空间，我们构建了一个名为FIND - Bench的新数据集。该数据集使用COCO图像，并包括用于集成定位和分割的新注释。这些注释是由GPT - 4生成的，尽管它不处理视觉输入，但可以直接将特定图像片段和注释ID与生成的描述（例如，<id>（金毛猎犬）...）相关联。这种独特的能力使得能够在交错上下文中创建用于检索和定位的训练和评估数据集。

综上所述，我们提出以下贡献：
1. 我们引入了FIND接口，该接口具有通用性、灵活性和可扩展性，适用于各种下游任务和基础模型。
2. 通过FIND的有效训练方案，创建了一个交错共享嵌入空间，用于连接基础模型。
3. 我们提出了一个新的基准FIND - Bench，其中包括用于交错分割和检索的新训练和评估真值。
4. 我们的模型在交错检索和定位方面达到了最先进的性能，并且在通用、交互式、基于语言的分割和图像 - 文本检索方面表现出更好或相当的性能。

## 2. 相关工作
### 基础模型
近年来，基础模型在不同领域如计算机视觉[47]、自然语言处理[39, 10, 4, 30]及其交互[1, 23, 44]方面迅速发展。例如，GPT - 3 [4]在自然语言理解和生成任务中取得了突破性进展。作为视觉基础模型，Florence [47, 42]可以轻松适应各种计算机视觉任务，如分类、检索、对象检测等。Flamingo [1]通过跨注意力的令牌融合，将强大的预训练视觉模型和语言模型连接起来。BLIP - 2 [23]提出了一种高效的预训练策略，通过轻量级Q - Former分两个阶段引导视觉 - 语言预训练。与之前的多模态方法（如Flamingo [1]、LLaVA [26]和Q - Former（BLIP - 2）[23]）不同，这些方法将视觉基础模型的输出输入到语言解码器中，并使用大语言模型（LLM）作为解释器，我们的目标是连接基础模型的嵌入，使LLM和视觉模型能够在嵌入空间中统一。

### 交错图像 - 文本理解
先前的工作在视觉问答、视觉对话、图像字幕和交错图像检索[20, 13, 1]等背景下探索了交错视觉理解。此外，最近的工作[48]探索了上下文检测，即将短语与句子中的视觉内容相关联。我们注意到，这些早期工作虽然揭示了图像理解的交错能力，但缺乏评估基准和完整的训练数据集。[51, 21, 2]提出了一个关于图像和文档级交错生成和理解的新基准，而对于交互式图像部分和短语之间的交错任务，尚无可用的基准。为此，我们引入了交错分割和交错检索任务，并精心设计了基准FIND - Bench，我们认为这对该领域至关重要。

### 图像理解
视觉变换器[16, 37, 40, 36, 41, 12, 15, 49, 33, 34]在一系列关键图像理解任务中占据主导地位，如图像检索、检测和分割。一些多模态方法[7, 24, 50]在检索任务中表现出良好性能。另一方面，开放词汇分割方法最近引起了广泛关注，包括通用分割[6, 53, 11]、通过主动整合用户输入分离对象的交互式分割[14, 19]以及从语言描述中定位对象片段的基于语言的分割[53, 52]。我们注意到，目前尚无单一模型能够同时实现图像级检索、像素级分割和交错视觉 - 语言理解。在这项工作中，我们提出FIND作为一个统一接口，能够支持上述所有任务，同时保持良好性能，并进一步实现交错分割和交错检索这两个新任务。我们通过连接基础模型的嵌入来统一这些任务。

## 3. 方法
诸如CLIP [32]、SAM [19]、LLaMA [38]等基础模型可以处理视觉或语言输入，进行推理、理解和生成。这些模型生成的嵌入包含丰富且结构化的信息[35, 3]，使其非常适合理解任务。与柏拉图表示假设[17]一致，我们认为基础模型可以轻松相互通信。因此，我们设计了FIND接口，将来自基础模型的视觉和语言嵌入投影到一个统一空间中。创建的空间增强了多模态和交错理解。

由于之前没有交错理解的基准，我们认为正式定义交错检索和分割问题并创建一个数据集来对其进行基准测试是有意义的。

### 3.1 FIND基准
我们的新基准支持两个任务：交错检索和交错定位。它评估数据集级和图像级的交错对齐，重点关注推理和匹配能力。此外，我们创建了训练和评估数据集，以进一步增强交错理解。

#### 3.1.1任务定义
- **交错检索**：一个交错条目（E）由一系列图像（I）、文本（T）和连接（C）组成，可以表示为 $E = < N_1, N_2,..., N_n | N_i \in {I, T, C}> $，其中 $<\cdot> $是一个有序序列。表??的底部清楚地说明了一个交错条目的示例。我们将交错检索的源域（ $D_s $）表示为 $D_s = {E_1, E_2,..., E_n} $，如图3.1（左）所示，目标域（ $D_t $）表示为 $D_t = {I_1, I_2,..., I_n} $，如图3.1（右）所示。交错检索的任务是为每个 $E \in D_s $找到最接近的条目 $I_* \in D_t $，不包括其自身。形式上，我们定义为 $\forall E \in D_s $， $I_* = \arg \max_{I \in D_t, I \notin E} sim(E, I) $。除非另有说明为交错文本检索，否则我们指的是如图3.1所示的交错视觉检索。
- **交错定位**：一幅图像包含一系列对象或片段（O），表示为 $I = {O_1, O_2,..., O_n} $。我们在图3.2的上部给出了面包店图像中对象的示例。这些对象构成了交错定位的目标域 $D_t^- = I = {O_1, O_2,..., O_n} $。与交错检索不同，在交错检索中交错条目构成源域，交错定位侧重于交错条目的每个组件，交错条目中的实体（N）构成源域。具体而言， $D_s = {N_1, N_2,..., N_n | N_i \in {I, T}} \subseteq E $。我们在图3.2的下部展示了一个交错条目分解的示例。交错定位的任务是为每个 $N \in D_s $找到最接近的条目 $O_* \in D_t $，不包括其自身。形式上，我们定义为 $\forall N \in D_s $， $O_* = \arg \max_{O \in D_t, O \notin N} sim(N, O) $。除非另有说明为交错文本定位，否则我们指的是如图3.2所示的交错视觉定位。

#### 3.1.2数据引擎
我们重用COCO数据集的图像和真实注释来创建FIND - Bench。在表1的第一部分，我们展示了用于GPT - 4上下文学习的输入数据。除了COCO真实注释外，我们还使用VLM模型（如LLaVA [26]）生成伪图像描述，以丰富信息。在表1的第二部分，我们给出了数据引擎的提示模板。该模板为表1第4部分中的交错字幕生成文本部分，提供与注释ID相关联的语言描述。这些ID对应的片段在表1所示的示例图像中以相同颜色突出显示。如3.1.1节所述，源组件和目标组件是互斥的。我们利用SEEM [53]强大的视觉理解能力，在条目中找到视觉组件的替换。检索和替换的视觉组件显示在表1的第4部分，确切的片段以与相应参考文本相同的颜色突出显示。例如，<the playing field>与COCO注释ID [3171126]相关联，并且在另一幅图像中找到类似的运动场（标记为蓝色）。通过这种方式，数据引擎可以为COCO数据集中的每幅图像生成全面的交错描述。这足以构建3.1.1节中介绍的交错检索和定位任务的 $D_s $和 $D_t $。

### 3.2 FIND方法
有了3.1节中介绍的基准来评估模型的交错视觉理解能力，我们现在介绍我们在多模态和交错理解中连接基础模型嵌入的方法。我们首先介绍任务统一和术语的预备知识。

#### 3.2.1预备知识
- **任务统一**：在这项工作中，我们关注以多模态和交错方式进行的检索、定位和分割。在图3中，我们展示了四个示例任务：交错检索、交错定位、交互式分割和通用分割。从抽象角度看，我们可以将所有视觉理解任务视为从源域到目标域匹配候选的问题。形式上，我们定义源域为 $D_s $，目标域为 $D_t $。 $D_s $或 $D_t $中的示例元素包括交错条目E、图像I、对象或片段o、文本T。对于每个视觉理解任务 $U(D_s, D_t) $，目标是为每个 $X \in D_s $找到最接近的 $Y \in D_t $。形式上我们写为： $\forall X \in D_s, Y^* = \arg \max_{Y \in D_t} sim(X, Y) $，其中X和Y分别是 $D_s $和 $D_t $的基本元素，sim $(X, Y) $表示X和Y之间的相似度。例如，在通用分割（图3.4）中， $D_s $是图像中所有对象（片段）的集合： $D_s = {\hat{O}_1,..., \hat{O}_{n_s}} $， $D_t $是类别名称的集合： $D_t = {T_1,..., T_{n_t}} $。对于 $D_s $中的每个对象o，我们将找到 $D_t $中相应的类别T。
- **术语**：这里我们将介绍重要的模型术语，包括提示（P）和查询（Q）。我们的模型支持三种输入：视觉（I）、语言（T）和交错视觉 - 语言（E）。视觉和语言基础模型预测这些输入的嵌入。如图4.1所示，通过对嵌入进行采样，我们获得视觉提示（ $P_I $）、语言提示（ $P_T $）和交错提示（ $P_E $）。此外，可训练查询使用随机参数初始化，将从提示中累积信息。例如，在通用分割中，对象查询（ $Q_O $）从视觉提示中收集信息。有趣的是，在FIND接口中，查询就像“桶”一样累积“水”（提示），如图4.1所示。

### 3.2.2模型管道
我们的模型设计为与任意一对视觉和语言基础模型接口。
- **提示和查询准备**：给定图像（I）、文本（T）和交错（E）输入，视觉编码器（ $F_v $）和语言编码器（ $F_l $）将对这些输入进行编码，得到嵌入序列M： $M_I = F_v(I), M_T = F_l(T), M_E = \{F_v, F_l\}(E) \quad (1) $，其中， $M \in \mathbb{R}^{n×d} $，n和d分别是嵌入数量和维度。与SEEM [53]类似，我们使用嵌入采样器为下游任务采样定制提示。示例采样策略包括下采样、区域的ROI池化以及交错提示的嵌入重新排列。采样过程不会改变嵌入分布。采样后，我们得到 $P_E, P_T, P_I,... = Emb\_Sample(M_I, M_T, M_E) $。此外，嵌入采样器负责从可学习查询池中采样查询（ $Q_E, Q_T, Q_I,... $）。我们允许在可学习查询的采样过程中重复。这些查询和提示是FIND接口的输入。从技术上讲，嵌入采样器通常是PyTorch中的插值或网格采样层。
- **FIND接口**：FIND接口主要由两个操作组成：内容注意力 $A_t $和条件注意力 $A_d $，如图4.3所示。内容注意力允许查询从相应提示中累积信息，而条件注意力使提示和查询能够内部推理（例如，对象查询上的自注意力以避免重复）。给定初始提示 $P^0 = {P_E^0, P_T^0, P_I^0,...} $和初始可学习查询 $Q^0 = {Q_E^0, Q_T^0, Q_I^0,...} $，内容注意力和条件注意力形式上定义为： $Q^{l + 1} = A_t(P^l, Q^l; [P^l \to Q^l]), Q^{l + 1}, P^{l + 1} = A_d(P^l, Q^l; [S^l \to Q^l], [P^l \to P^l]) \quad (2) $，其中 $S^l \subseteq {P^l, Q^l} $是查询和提示的子集，表示注意力掩码。例如， $[P \to Q] $表示Q在注意力期间能够关注P。这样，提示充当信息源，查询充当桶。在图4.2中，我们展开了FIND接口支持的一些任务的提示和查询。
- **投影**：FIND接口的输出是一系列查询： $Q^L = {Q_O^L, Q_T^L, Q_I^L, Q_E^L,...} $。然后，我们使用线性层、MLP将查询投影到语义和像素空间，分别用于语义和像素投影。语义和像素查询计算为 $Q^s = MLP_s(Q^L) \in \mathbb{R}^{n_t×d} $和 $Q^p = MLP_p(Q^L) \in \mathbb{R}^{n_t×d} $，其中 $n_t $是总实例数量，d是嵌入维度。语义输出用于检索、类别映射等，而像素输出用于掩码预测。
- **任务头**：有了投影后的查询，如3.2.1节所述，每个理解任务可以表示为一个相似性映射过程。形式上，给定初始图像嵌入 $M_I \in \mathbb{R}^{n_p×d} $（其中 $n_p $是像素数量），可以计算分割结果（掩码）。相似性分数（分数）可以直接从 $Q^s $计算。每个任务的输出是掩码、分数的子集。掩码 =  $Q^p×M_I^{\top} \in \mathbb{R}^{n_t×n_p} $，分数 =  $Q^s×Q^{s\top} \in \mathbb{R}^{n_t×n_t} $。FIND通过线性组合损失进行训练，这些损失包括全景分割、基于语言的分割、交互式分割、图像 - 文本检索、来自同一图像的视觉实体的交错检索和交错定位。我们在附录中展示了损失细节。

## 4. 实验
### 数据集
我们使用COCO [25]作为主要的训练和评估数据集，它涵盖了多种注释类型。我们利用了COCO - 全景、Ref - COCO [45, 28, 29]、COCO - Karpathy [18]的注释，以及使用FIND - Bench中的数据引擎生成的新数据集。我们生成了两组新注释，包括COCO - 实体和COCO - 段落，详细统计信息如下表所示：

|数据集|训练-图像|训练-字幕|训练-实体|评估-图像|评估-字幕|评估-实体|实体-掩码|实体-短语|实体-图像|平均实体/图像|
|---|---|---|---|---|---|---|---|---|---|---|
|COCO - 实体|118189|353219|1104907| 4990|4990|15305|✓|✓|✓| 4 |
|COCO - 段落|-| - | - |4981|4981|22569|✓|✓|✓|7|

### 设置
我们在三种不同模型大小上对我们的方法进行基准测试：小型（FocalNet）、基础型（Davit - d3）和大型（Davit - d3）。视觉骨干网络是固定的，并重用X - Decoder的预训练权重，除非另有说明为SAM。语言骨干网络是固定的LLaMA - 7B，除非另有说明为UniCL。在训练期间，除非另有说明，我们在所有任务上联合训练FIND接口。

### 指标
我们使用标准评估指标评估所有任务。对于新提出的交错检索，我们使用IR@5和IR@10（交错到图像检索准确率在排名5/10）。对于交错定位，我们根据预测的交错掩码和真实掩码之间的像素级IoU（cIoU）和图像级IoU（mIoU）进行评估。

### 基线
我们使用ImageBind [13]、FROMAGe [20]、BLIP2 [23]作为交错检索任务的基线；使用Grounding - SAM [27]、SEEM [53]作为交错定位的基线。我们声称尽最大努力设计基线评估协议以实现最佳性能。

### 4.1主要结果
在主要实验中，我们专注于评估FIND在摘要中声称的通用性、可交错性和可扩展性能力。
- **通用分割、定位和检索**：表2将FIND与强大的基线在通用分割任务上进行比较，包括全景分割、实例分割和语义分割。此外，我们在指代分割（RefCOCO - g：一个句子与一个实例相关联）和基于语言的分割（COCO - 实体和COCO - 段落：一个句子与多个实例相关联）设置中展示了分割能力。此外，我们还在COCO上的三种不同真实类型上对FIND在图像 - 文本检索中的性能进行基准测试，其中分割（Karpathy、实体和段落）的平均句子长度逐渐增加。以下是主要发现：
    - **实例分割结果突出**：我们使用大型视觉编码器的方法优于类似模型，如Mask2Former、X - Decoder和SEEM，比额外使用可变形卷积的Mask2Former（L）性能高出2.2个百分点。值得注意的是，Mask2Former和FIND的分割训练数据相同。性能提升可能源于我们统一的分割和定位管道，它从每个域的语义真实中相互受益。
    - **基于语言和指代分割的相互受益**：在FIND中，我们使用查询和提示统一了基于语言和指代分割。如表2所示，我们的模型在COCO - 实体和COCO - 段落上达到了最先进的性能，并在Ref - COCO - g数据集上优于强大的基线。
    - **统一设置下交互式分割性能得以保留**：与仅在图像任务上训练的SEEM不同，FIND还在图像 - 文本任务（如图像 - 文本检索）上进行训练。通过巧妙设计查询、提示和注意力机制，交互式分割和图像 - 文本检索的训练不会相互干扰。因此，我们的方法能够实现具有竞争力的性能（即FIND 88.5/89.5/77.4与SEEM 88.5/89.6/76.5相比）。
    - **图像 - 文本检索结果欠佳**：图像 - 文本检索性能欠佳是由于微调期间的批量大小。对X - Decoder的初步实验表明，不同分辨率（如图像为1024，语言为224）在任务之间不能很好地泛化。因此，FIND在所有任务中使用相同分辨率进行训练。在表2中，模型在所有任务中要么是384x384分辨率，批量大小为384，要么是1024x1024分辨率，批量大小为192。其他表格显示了640x640训练分辨率和192批量大小的结果。
- **在视觉和语言模态上可交错**：在表3中，我们在FIND - Bench中的交错数据集级和图像级理解任务上评估FIND。在COCO - 实体和COCO - 段落列中，我们有0.5的概率用视觉参考替换文本实体，与表2不同，这些列纯粹基于语言数据进行评估。
    - **交错分割**：我们使用SEEM模型构建了一个交错分割基线。由于SEEM不支持以交错格式制定定位任务，我们只是简单地使用SEEM的交互式或定位功能分别推断视觉和文本实体。如表3所示，FIND在交错分割上优于SEEM，在COCO - 实体和COCO - 段落上的cIoU指标上高出约8个点。
    - **交错检索**：我们还在FIND上探索了跨图像交错检索。由于交错参考对象来自同一验证集，IR@1没有意义，因此我们在这种设置下报告IR@5和IR@10。对于ImageBind和BLIP - 2，我们使用文本、句子和图像的集成分数。遵循FROMAGe的交错图像 - 文本检索设置，我们的性能明显高于基线，证明了我们交错共享嵌入空间的有效性。
    - **通用分割**：除了使用类别名称或固定索引的经典评估外，我们用类别描述（长描述）或视觉提示（每个类别的对象查询的平均特征）替换类别。利用大语言模型，FIND在基于描述的分割方面表现出色，受益于更平滑的表示和更好的长上下文处理。我们还展示了FIND在视觉上下文设置中的有效性。
- **可扩展到任意基础模型和任务**：在主要实验中，我们使用X - Decoder作为视觉编码器，LLaMA作为语言编码器，在所有任务上都表现出令人信服的性能。X - Decoder已被训练为匹配视觉和语言嵌入，然而，SAM仅在分割数据上进行训练，没有任何语义含义。因此，我们使用SAM作为消融视觉基础模型，研究使用语义数据训练的视觉编码器的重要性。对于语言编码器，我们采用与Bert大小相同的UniCL，研究标准语言编码器和大语言模型编码器之间的差异。如表4所示，除了LLaMA在长描述推理方面非常有效外，UniCL和LLaMA在使用X - Decoder作为视觉编码器时通常具有非常相似的性能。尽管在训练接口后，SAM在语义理解方面的性能比其对应模型X - Decoder差得多，但我们的方法也表明，无需对SAM进行任何修改，它适用于通用、基于语言的分割和图像 - 文本检索等语义理解任务。

### 4.2消融研究
我们从两个角度对我们的方法进行消融研究：（1）统一管道中每个任务的有效性如何？（2）使用大语言模型表示的中间层的有效性。
- **独立任务有效性**：我们通过在表5中逐渐移除任务来评估任务有效性。移除图像 - 文本检索会显著降低交错检索性能。进一步移除定位任务会降低基于实体的定位性能。由于交错定位与交互式分割相关，移除它也会降低交错分割性能。最后，仅训练全景分割会产生与其他设置相似的性能，表明统一接口与基本任务训练的一致性。
- **改变大语言模型的特征嵌入层**：大语言模型处理语言标记，输入和输出层附近的嵌入语义性较差。我们假设中间层与视觉嵌入更好地对齐。表5显示了使用从层 - 1（输出）到 - 30（输入）的嵌入在任务上的性能。层 - 12的嵌入表现最佳，而顶部和底部层在COCO - Karparthy分割上的图像 - 文本检索中表现较差。因此，我们在整个论文中使用LLaMA的层 - 12嵌入。

### 4.3演示结果
- **交错相册搜索**：我们FIND方法中的查询支持线性复杂度的交错相册搜索。给定图像、交错或文本输入，我们的模型可以检索和分割相册中的所有照片。下面，我们展示一个使用COCO验证集作为搜索空间的示例。“is standing on the rock under the trees”。
- **交错视频定位**：我们可以将视频帧定位问题表述为图像 - 文本检索任务。这使我们能够根据给定指令推理和识别相应对象，如下所示。我们相信FIND对机器人导航有用。“Can you water the green leaves using”。
- **3D特征场**：基础模型嵌入用于为机器人操作、定位和推理创建3D特征场。我们相信，具有像素级理解能力的交错嵌入空间在3D特征场中具有巨大潜力。下面，我们比较使用FIND嵌入和CLIP嵌入训练的场景。FIND 3D特征场和CLIP 3D特征场。

### 结论和未来工作
这项工作介绍了FIND接口，这是一个用于对齐基础模型嵌入的通用接口，以及用于训练和评估的FIND基准。在4.3节中，我们展示了交错相册搜索、视频定位和3D特征场等潜在应用。这些示例清楚地说明了我们的模型在个性化基础模型和机器人技术方面的潜力。


### 参考资料
- 标题：Interfacing Foundation Models’ Embeddings
- 作者：Xueyan Zou, Linjie Li, Jianfeng Wang, Jianwei Yang, Mingyu Ding, Junyi Wei, Zhengyuan Yang, Feng Li, Hao Zhang, Shilong Liu, Arul Aravinthan, Yong Jae Lee, Lijuan Wang
- 单位：UW - Madison, Microsoft, UC Berkeley, HKUST, Tsinghua University
- 标签：计算机视觉、自然语言处理、基础模型、多模态、交错任务
- 概述：本文提出FIND接口，用于对齐基础模型嵌入，支持交错分割、定位和检索任务，创建了FIND - Bench基准，在多个任务上评估展示了良好性能及潜力。
- 链接：https://arxiv.org/pdf/2312.07532v2
- 标题翻译：基础模型嵌入的接口
- 摘要: 本文提出FIND接口，旨在利用基础模型嵌入扩展输出空间，实现交错理解与推理。该接口具有通用性、交错性和可扩展性，通过统一训练创建交错共享嵌入空间，并引入FIND - Bench基准。实验表明，FIND在多个任务上表现出色，包括通用分割、交错分割和检索等，且在不同模型设置下均取得良好效果，展示了其在个性化基础模型和机器人技术等方面的潜力，但也存在数据量有限等局限性。
