# 当强化学习 “邂逅” 自然语言，解锁策略学习新范式

**摘要**：强化学习（RL）在学习决策任务策略方面展现出了卓越的能力。然而，RL常常受到诸如样本效率低、缺乏可解释性和监督信号稀疏等问题的困扰。为了解决这些局限性，我们从人类学习过程中获取灵感，引入了自然语言强化学习（NLRL），它创新性地将RL原理与自然语言表示相结合。具体而言，NLRL在自然语言空间中重新定义了RL概念，如任务目标、策略、价值函数、贝尔曼方程和策略迭代。我们展示了如何借助GPT - 4等大语言模型（LLM）的最新进展来实际实现NLRL。在表格型马尔可夫决策过程（MDP）上的初步实验证明了NLRL框架的有效性、高效性和可解释性。

## 1. 引言
强化学习构建了一个数学框架，涵盖了关键的决策要素。它通过累积奖励的概念量化任务目标，用概率分布制定策略，通过数学期望表达价值函数，并通过状态转移和奖励函数对环境动态进行建模。这个框架有效地将策略学习问题转化为一个优化问题。

尽管近年来RL取得了显著成就，但其框架的局限性仍然存在诸多重大挑战。例如，RL面临样本效率问题——RL算法与任务无关，不利用任何先验知识，需要大规模和广泛的采样来了解环境。RL也缺乏可解释性。尽管像AlphaZero（Silver等人，2017）这样的模型在掌握围棋等复杂游戏方面表现出超人的性能，但其决策过程的基本战略逻辑即使对专业玩家来说也难以捉摸。此外，RL的监督信号是一维标量值，与文本和图像等信息丰富的数据集上的传统监督学习相比，要稀疏得多。这也是RL训练不稳定的原因之一（Zheng等人，2023；Andrychowicz等人，2020）。

这些局限性促使我们寻求一种受人类学习过程启发的新框架。与RL算法对决策组件进行数学建模不同，人类倾向于通过自然语言进行相对模糊的操作。首先，自然语言使人类能够利用基于文本的先验知识，这在学习新任务时大大提高了样本效率。其次，人类具有独特的能力，能够在决定行动之前用自然语言表达明确的战略推理和想法，使得他们的过程即使不是完成任务的最有效方法，也能被他人完全理解。第三，自然语言数据包含有关思考、分析、评估和未来规划的信息。它可以提供信息密度高的信号，远远超过传统RL奖励信号中的信息密度。

受人类学习过程的启发，我们提出了自然语言强化学习（NLRL），这是一种新的RL范式，创新性地将传统RL概念与自然语言表示相结合。通过将关键的RL组件——如任务目标、策略、价值函数、贝尔曼方程和广义策略迭代（GPI）（Sutton和Barto，2018）——转化为其自然语言等价物，我们利用语言的直观力量来封装复杂的决策过程。这种转化得益于大语言模型（LLM）的最新突破，这些模型具有理解、处理和生成基于语言的信息的类人能力。我们在表格型MDP上的初步实验验证了NLRL框架的有效性、高效性和可解释性。

## 2. 强化学习预备知识
强化学习将决策问题建模为马尔可夫决策过程（MDP），由状态空间 $S$、动作空间 $A$、概率转移函数 $P: S \times A \times S \to [0,1]$、折扣因子 $\gamma \in [0,1)$和奖励函数 $r: S \times A \to [-R_{max}, R_{max}]$定义。RL的目标是学习一个策略 $\pi: S \times A \to [0,1]$，它衡量在给定状态 $s$下采取动作 $a$的概率： $\pi(a | s)=Pr(A_t = a | S_t = s)$。在决策任务中，最优策略倾向于最大化预期折扣累积奖励： $\mathbb{E}_{\pi}[\sum_{t = 0}^{\infty} \gamma^t r(s_t, a_t)]$。状态 - 动作价值函数和状态价值函数是通过RL目标的代理来评估状态和动作的两个关键概念： $Q_{\pi}(s_t, a_t)=\mathbb{E}_{(s, a)_{t + 1:\infty} \sim P_{\pi}}[\sum_{i = t}^{\infty} \gamma^{i - t} r(s_i, a_i) | s_t, a_t]$， $V_{\pi}(s_t)=\mathbb{E}_{(s, a)_{t + 1:\infty} \sim P_{\pi}}[\sum_{i = t}^{\infty} \gamma^{i - t} r(s_i, a_i) | s_t]$，其中 $P_{\pi}$是给定策略 $\pi$和动态转移 $P$的轨迹分布。

给定 $V_{\pi}(s_t)$的定义，时间上相邻状态的值之间的关系可以推导为贝尔曼期望方程。这是一步贝尔曼期望方程： $V_{\pi}(s_t)=\mathbb{E}_{a_t \sim \pi_{\theta}}[r(s_t, a_t)+\gamma \mathbb{E}_{s_{t + 1} \sim p(s_{t + 1} | s_t, a_t)}[V_{\pi}(s_{t + 1})]], \forall s_t \in \mathcal{S}$。对于 $Q_{\pi}(s, a)$也可以推导出类似的方程。基于这些基本的RL定义和方程，我们可以说明在GPI中如何进行策略评估和策略改进。

## 2.1 策略评估
策略评估过程的目标是估计状态价值函数 $V_{\pi}(s)$或状态 - 动作价值函数 $Q_{\pi}(s, a)$。为了简单起见，在下面的说明中我们仅使用 $V_{\pi}(s)$。两种常见的价值函数估计方法是蒙特卡罗（MC）估计和时间差分（TD）估计（Sutton，1988）。MC估计通过对轨迹进行蒙特卡罗采样来构建无偏估计： $V_{\pi}(s_t) \approx \frac{1}{k} \sum_{n = 1}^{K}[\sum_{i = t}^{\infty} \gamma^{i - t} r(s_i^n, a_i^n)]$。TD估计依赖于公式（1）中所示的时间关系来构建估计： $V_{\pi}(s_t) \approx \frac{1}{k} \sum_{n = 1}^{K}[r(s_t, a_t^n)+\gamma V_{\pi}(s_{t + 1}^n)]$，这可以看作是对下一状态价值函数的自举。

## 2.2 策略改进
策略改进过程旨在根据策略评估的结果更新和改进策略。具体来说，它用新策略 $\pi_{new}$替换旧策略 $\pi_{old}$，以使预期回报增加： $V_{\pi_{new}}(s_0) \geq V_{\pi_{old}}(s_0)$。在具有小的离散动作空间的环境中，这种改进可以通过在每个状态下贪婪地选择使 $Q_{\pi_{old}}(s, a)$最大化的动作来实现： $\pi_{new}(\cdot | s)=\underset{\pi(\cdot | s) \in \mathcal{P}(A)}{\arg\max} \mathbb{E}_{a \sim \pi}[Q_{\pi_{old}}(s, a)], \forall s \in \mathcal{S}$。另一种改进方法涉及应用策略梯度上升（Sutton等人，1999）。它用 $\theta$参数化策略 $\pi_{\theta}$。解析策略梯度可以推导如下： $\left.\nabla_{\theta} V(\pi_{\theta})\right|_{\theta = \theta_{old}}=\mathbb{E}_{(s, a) \sim P_{\pi_{\theta_{old}}}}[\left.\nabla_{\theta} \log \pi_{\theta}(a | s) Q_{\pi_{\theta_{old}}}(s, a)\right|_{\theta = \theta_{old}}]$。通过选择一个相对较小的步长 $\alpha>0$来进行梯度上升： $\theta_{new}=\theta+\alpha \nabla_{\theta} V_{\pi_{\theta}}(s_0)|_{\theta = \theta_{old}}$，我们可以保证策略改进： $V_{\pi_{new}}(s_0) \geq V_{\pi_{old}}(s_0)$。

## 3. 自然语言强化学习
与传统RL中使用的精确统计模型不同，人类通常以自然语言的形式构建所有元素——包括任务目标、价值评估和战略策略。本节旨在模仿人类如何使用自然语言进行决策任务，使其与传统RL中的概念、定义和方程保持一致。由于自然语言的固有模糊性，这里给出的方程并非严格从数学定义推导而来。它们是类比的，基于对原始RL概念的实证见解。我们将严格的理论留待未来的工作。

## 3.1 定义
我们从自然语言RL中的定义和方程开始，以模拟人类的行为。我们在图1中提供了示例，涵盖了我们将在本节中讨论的大多数概念。

### 基于文本的MDP
为了在自然语言空间中进行RL，我们首先需要将传统MDP转换为基于文本的MDP，它利用文本和语言描述来表示MDP的基本概念，包括状态、动作和环境反馈（状态转移和奖励）。

### 语言任务指令
人类通常定义一个自然语言任务指令 $T_L$，如“到达目标”或“打开门”。然后，我们用 $F$表示一个人类度量，它根据轨迹描述 $D_L(\tau_{\pi})$（一个可以将轨迹分布 $\tau_{\pi}$转换为相应语言描述 $D_L(\tau_{\pi})$的语言描述符）来衡量任务指令的完成程度。基于此符号，NLRL的目标被重新表述为： $\max_{\pi} F(D_L(\tau_{\pi}), T_L)$。我们试图优化策略，使轨迹分布 $\tau_{\pi}$的语言描述能够显示任务指令的高完成度。

### 语言策略
人类不是直接对动作概率进行建模，而是通过战略思想、逻辑推理和规划来确定动作。因此，我们将语言策略表示为 $\pi_L(a, c | s)$，它将首先生成这样的思维过程 $c$，然后生成最终的动作概率 $\pi(a | s)$。

### 语言价值函数
与传统RL中 $Q$和 $V$的定义类似，人类利用语言价值函数，依靠自然语言评估来评估策略的有效性。语言状态价值函数 $V_{\pi}^L$和语言状态 - 动作价值函数 $Q_{\pi}^L$定义为： $Q_{\pi}^L(s_t, a_t)=D((s, a)_{t + 1:\infty} \sim P_{\pi} | s_t, a_t, T_L)$， $V_{\pi}^L(s_t)=D(a_t, (s, a)_{t + 1:\infty} \sim P_{\pi} | s_t, T_L)$。

给定当前状态 $s_t$或状态 - 动作 $(s_t, a_t)$， $Q_{\pi}^L$和 $V_{\pi}^L$利用语言描述而不是标量值来展示策略对实现任务目标 $T_L$的有效性。语言价值函数在直观上富含价值信息并增强了可解释性，而不是传统的基于标量的价值。它可以从不同角度表示评估结果，包括潜在逻辑/思想、对未来结果的预测/分析、不同动作之间的比较等。

### 语言贝尔曼方程
在贝尔曼方程中，状态评估值 $V(s_t)$可以分解为两部分。首先是中间变化，包括立即动作 $a_t$、奖励 $r_t$和下一状态 $s_{t + 1}$。其次是对下一状态的状态评估 $V(s_{t + 1})$。基于这种分解直觉，我们根据分解原理引入语言贝尔曼方程公式（6）： $V_{\pi}^L(s_t)=G_1^{a_t, s_{t + 1} \sim P_{\pi}}(G_2(d(a_t, r(s_t, a_t), s_{t + 1})), V_{\pi}^L(s_{t + 1}))), \forall s_t \in \mathcal{S}$，其中 $d(a_t, r(s_t, a_t), s_{t + 1}))$描述了中间变化的语言描述， $G_1$和 $G_2$是两个信息聚合函数。具体来说， $G_2$模仿原始贝尔曼方程中的“ $+$”操作，聚合来自中间变化描述和给定 $a_t$和 $s_{t + 1}$的未来评估的信息。 $G_1$负责期望操作 $E$，通过从轨迹分布 $P_{\pi}$中采样聚合来自不同 $(a_t, s_{t + 1})$对的信息。

## 3.2 语言广义策略迭代
给定定义和方程，在这部分中，我们介绍如何进行语言GPI。参考图1中的语言GPI示例。

### 3.2.1 语言策略评估
语言策略评估旨在估计每个状态的语言价值函数 $V_{\pi}^L$和 $Q_{\pi}^L$。我们展示两种经典估计方法：MC和TD估计在语言策略评估中的工作方式。

- **语言蒙特卡罗估计**：从状态 $s_t$开始，对给定策略 $\pi$的文本展开（即 $K$个完整轨迹 ${a_t, (s, a)_{t + 1:\infty}}$）进行MC估计。由于我们不能在语言空间中进行平均操作，我们改为利用语言聚合器/描述符 $G_1$在有限轨迹上聚合信息，近似预期评估： $V_{\pi}^L(s_t) \approx G_1(\{a_t^n, (s, a)_{t + 1:\infty}^n\}_{n = 1}^{k})$。

- **语言时间差分估计**：TD估计主要依赖于公式（6）中所示的一步语言贝尔曼方程。与MC估计类似，我们聚合 $K$个一步样本来近似预期评估： $V_{\pi}^L(s_t) \approx G_1(\{G_2(d(s_t, a_t^n, r(s_t, a_t^n), s_{t + 1}^n), V_{\pi}^L(s_{t + 1}^n)))\}_{n = 1}^{K}), \forall s_t \in \mathcal{S}$。

语言MC估计不受估计“偏差”的影响，因为它直接利用来自完整轨迹的样本。然而，考虑到长期未来步骤的显著变化，MC方法容易出现高“方差”。这种可变性给公式（7）中的语言聚合器 $D$从不同轨迹中有效提取关键信息带来了挑战。相反，虽然下一状态评估 $V_{\pi}^L(s_{t + 1})$的不准确会给TD估计带来估计“偏差”，但它们通过丢弃未来变化有效地降低了“方差”。 $G_1$和 $G_2$只需要进行有限变化的简单一步信息聚合。

### 3.2.2 语言策略改进
与传统策略改进类似，语言策略改进的动机也是选择使人类任务完成度函数 $F$最大化的动作： $\pi_{new}(\cdot | s)=\underset{\pi(\cdot | s) \in \mathcal{P}(A)}{\arg\max} F(Q_{\pi_{old}}^L(s, a), T_L), \forall s \in \mathcal{S}$。

如前所述， $F$通常是人类对任务完成度的度量，难以量化并进行 $\arg\max$操作。考虑到 $F$在很大程度上依赖于人类先验知识，我们不是通过数学优化它，而是利用语言分析过程 $I$进行策略优化并选择动作： $\pi_{new}(\cdot | s), c = I(Q_{\pi_{old}}^L(s, a), T_L), \pi(\cdot | s) \in \mathcal{P}(A), \forall s \in \mathcal{S}$。

语言策略改进通过进行战略分析 $c$来生成思维过程 $c$，并确定最有希望的动作作为新策略 $\pi_{new}(\cdot | s)$。这种分析主要基于人类对语言评估 $Q_{\pi_{old}}^L(s, a)$与任务目标 $T_L$之间的相关性判断。

## 3.3 利用大语言模型的实际实现
第3节展示了NLRL的理念：将RL关键概念转化为其自然语言对应物。为了实际实现这些关键概念，我们需要一个能够理解、处理和生成语言信息的模型。经过大规模人类语言和知识语料库训练的大语言模型，可以成为帮助模拟人类行为和实现这些语言RL组件的自然选择。

- **大语言模型作为策略 $(\pi_L)$**：许多工作采用大语言模型作为决策智能体（Wang等人，2023a；Feng等人，2023a；Christianos等人，2023；Yao等人，2022），并结合思维链过程（Wei等人，2022b）。通过设置适当的指令，大语言模型可以利用自然语言描述其确定动作的潜在思维，类似于人类的战略思维。

- **大语言模型作为信息提取器和聚合器 $(G_1, G_2)$**：大语言模型可以成为强大的信息总结器（Zhang等人，2023）、提取器（Xu等人，2023）和聚合器，帮助我们融合中间变化和未来语言评估，以进行语言价值函数估计。一个核心问题是确定我们希望大语言模型提取和聚合哪种信息。受可解释强化学习领域中Das等人（2023）、Sreedharan等人（2020）、Schut等人（2023）、Hayes和Shah（2017）的工作启发，我们认为概念可以是核心。我们采用Das等人（2023）中的说明，即概念是一种基于人类领域知识的通用、面向任务目标的高级抽象。例如，在最短路径查找问题中，路径距离和可用路径集是两个概念，它们（1）是轨迹的高级抽象，在人类先验知识中预先定义，（2）通常适用于不同状态，（3）与最终任务目标直接相关。基于这些动机，大语言模型将尝试聚合和提取特定领域的概念，以形成价值目标信息。这些概念可以由人类先验知识预先定义，也可以由大语言模型本身提出。

- **大语言模型作为价值函数近似器 $(D_L, Q^L, V^L)$**：价值函数近似的关键思想（Sutton等人，1999）是用参数化函数而不是表格表示来表示价值函数。如今，深度强化学习通常选择神经网络，它以状态作为输入并输出一维标量值。对于NLRL， $D_L$、 $Q^L$、 $V^L$的语言价值函数近似可以自然地由（多模态）大语言模型处理。大语言模型可以接收任务状态的特征，如低维统计数据、文本和图像，并输出相应的语言价值判断和描述。对于大语言模型的训练，我们采用上述概念提取器/聚合器来形成MC或TD估计（在3.2.1节中），可用于微调大语言模型以获得更好的语言评估器。

- **大语言模型作为策略改进算子 $(I)$**：借助思维链过程和人类先验知识，大语言模型能够通过对语言评估 $Q_{\pi_{old}}^L(s, a)$与任务目标 $T_L$的相关性进行语言分析 $c$，更好地确定最有希望的动作 $\pi_{new}(\cdot | s)$。其基本思想也与一些近期工作（Kwon等人，2023；Rocamonde等人，2023）一致，这些工作利用大语言模型或视觉 - 语言模型作为奖励——它们可以准确地建模相关性。

## 3.4 关于其他RL概念的讨论
为了说明该框架的多功能性，我们展示了其他一些基本RL概念如何在NLRL中构建的示例。

- **TD - λ（Sutton，1988）**：公式（6）考虑了价值函数的一步分解，或者在传统RL的背景下，是TD(1)的情况。一个自然的扩展是对语言价值函数进行 $n$步扩展，并形成TD(n)估计。通过进一步聚合这些不同的估计（从1到 $n$），我们得到了NLRL的TD - λ估计。在语言价值函数下的TD - λ涉及将不同的语言评估聚合到最终的语言估计中。

- **探索**：探索是RL中的一个基本挑战。传统RL方法通常采用引入随机性的策略，如 $\epsilon$-贪婪方法或熵正则化（Sutton和Barto，2018），以促进探索。相比之下，NLRL提供了一种由不确定性引导的探索途径。例如，NLRL中的语言价值函数善于纳入状态不确定性模型。这允许在语言策略改进过程中进行更深入的分析过程，其中策略可以被细化以减少这种不确定性。

- **基于情节和检索的RL**：基于情节和检索的RL（Goyal等人，2022；Blundell等人，2016）的概念是对过去的轨迹保持明确记录，这些记录将用于在新状态下增强决策能力。这与大语言模型中的检索增强生成（RAG）（Lewis等人，2020）的思想一致。通过在过去的轨迹上利用RAG，NLRL也可以扩展到基于检索的RL。

## 4. 实验
我们在表格型MDP上进行了初步实验，作为概念验证。更多实验结果、设置和提示请参考附录A。

## 4.1 热身：文本网格世界中的语言策略评估
我们从最短路径查找问题开始，这是一个常用于说明基于动态规划的策略迭代算法的示例。如图3（迭代0）所示，这是一个 $4×4$的网格世界，目标是通过最短路径到达 $(0,0)$和 $(3,3)$的两个灰色网格。我们将这个环境转换为一个文本游戏，其中我们利用自然语言来表示状态、转移和环境反馈，如路径惩罚和到达目标的终止条件。我们将初始策略设置为随机策略，从所有可用动作的均匀分布中采样动作： $\pi(a | s)=\frac{1}{|A|}, \forall a \in A, s \in S$。我们对这个策略进行语言策略评估，类似于原始策略迭代算法中的策略评估过程。

我们将所有状态的语言评估描述初始化为“无评估信息”，除了终端状态，终端状态用与其终端性质特定的预定义描述进行初始化。与传统策略评估过程类似，每个语言策略评估迭代遍历所有非终端状态。对于每个状态 $s$和动作 $a$，它利用公式（6）中的一步贝尔曼方程计算新的语言状态 - 动作价值函数 $Q^L(s, a)$和 $V^L(s)$。这个评估进行了四次迭代，以观察语言状态 - 动作价值函数在迭代过程中的演变。我们使用GPT - 4 - preview - 1106模型作为语言贝尔曼更新的信息聚合器。此外，通过将坐标转换为字母，我们确保GPT - 4仅依赖于环境反馈进行信息收集，从而消除对其固有知识库的依赖。

在图2中，我们展示了在 $(0,3)$的状态 $g$上的语言评估结果在不同迭代中的变化，作为一个代表性示例。有关不同状态的全面结果，请参阅附录A。我们的分析得出了三个关键见解。首先，尽管我们提示GPT - 4模型进行策略评估，但它本质上进行了策略改进过程，选择了最有利的动作选项。这表明GPT - 4具有语言策略增强能力，因为它解释语言评估和任务目标。其次，最终迭代中的评估结果正确识别了每个状态的最优动作。这证实了语言策略评估和迭代过程的有效性和高效性，仅四次策略评估迭代就足以产生精确的语言评估和相应的最优动作，明显优于传统的策略迭代方法。最后，语言评估结果阐明了目标相关信息的逐渐传递，图2中的示例说明了信息如何在三次评估迭代中从目标流向状态 $g$。此外，图3用于直观地描绘信息在迭代过程中的流动。

## 4.2 随机环境中的语言策略评估和改进
在我们的第二个实验中，我们探索了Brockman等人（2016）提出的Frozen - Lake环境，这是一个网格世界，旨在引导智能体到达目标同时避开湖泊中的洞，由于状态转移的随机性而具有挑战性。按照Sheng等人（2023）的框架，我们将这个环境转换为基于文本的格式。我们的方法包括四次语言策略迭代，每次迭代包括一个评估和一个改进阶段。

我们的初步实验表明，直接提示方法（在我们的第一个实验中使用）在Frozen - Lake任务中很容易使策略迭代过程失败。这主要是因为大量的轨迹变化很容易分散大语言模型的注意力并破坏聚合过程。为了解决这个问题，我们采用了一种更明确的聚合方法：3.3节中讨论的基于概念的语言价值函数和信息提取。我们预定义了表1中列出的几个关键概念，并提示GPT - 4 - 0125 - preview模型根据这些概念生成评估并聚合信息。

图4展示了语言价值函数和语言策略改进过程的一个示例。价值函数示例清楚地解释了为什么定义与任务相关的概念可以促进信息聚合。GPT - 4可以轻松进行明确的概念聚合：它可以通过将最新动作附加到最左边来更新路径信息，或者通过传输目标和洞的坐标来提取环境信息。

在表2中，我们使用Frozen - Lake的原始奖励函数提供了每次迭代的策略价值。该表呈现出总体上升趋势，验证了我们语言策略迭代过程的有效性。然而，该系统仅达到最优策略效率的60%。对语言价值函数的进一步分析指出了问题所在：即使是最先进的GPT - 4模型也不能免受幻觉问题的影响，偶尔会生成不准确的信息。这些不准确信息可以通过连续的贝尔曼更新累积，导致错误的评估和决策。正在努力通过改进提示策略来减轻这种不稳定性。

## 5. 相关工作
- **基于语言模型的自主智能体**：受大语言模型在复杂推理和规划场景中强大的涌现能力的启发（Brown等人，2020；Wei等人，2022a；Anil等人，2023；OpenAI，2023；Feng等人，2023b），基于语言模型的自主智能体领域见证了利用大语言模型进行高级规划目的的增长趋势。对于文本智能体，ReAct（Yao等人，2022）通过对中间目标和任务特定生成的思维链规划，利用少样本提示。Reflexion（Shinn等人，2023）建立在ReAct（Yao等人，2022）的基础上，具有自我反思能力，称为言语强化学习，根据在线反馈生成提示。这在某种程度上与我们的语言蒙特卡罗估计设置一致，我们通过对多个轨迹进行采样来提取核心信息/概念。另一系列工作，如Zhang等人（2023）、Xu等人（2023），利用大语言模型进行总结和提取信息，以帮助融合中间变化。在具身智能体研究中（Duan等人，2022），例如机器人学习的研究展示了大语言模型在分层规划和子目标生成方面的潜力（Huang等人，2022；Liang等人，2023）。

- **可解释强化学习**：可解释强化学习的主要目的之一是为非人工智能专家自动寻求解释。例如，针对顺序决策任务的基于概念的解释方法。Ji等人（2023）通过对学习到的人类可解释特征进行聚类，为3D动作识别卷积网络提供了基于概念的解释。Sreedharan等人（2020）根据状态前提条件和动作成本制定了基于概念的解释，代表用户与给定状态相关的任何事实陈述。类似地，Hayes和Shah（2017）使用逻辑公式来总结策略。此外，Das等人（2023）训练了一个用于状态 - 动作对和基于概念的解释的联合嵌入模型。

- **从语言反馈中学习**：我们的工作也与从语言反馈中学习相关（Cheng等人，2023）。Cheng等人（2023）专注于如何对算法进行基准测试，而我们旨在提出一种新的算法框架。在机器人和自主智能体领域，已经研究了如何将对RL智能体活动的反馈作为学习信号的一部分，除了任务奖励之外，其中大部分努力致力于引出二元偏好反馈（Biyik和Sadigh，2018）或从人类获得基于排名的标签（Sumers等人，2021）。我们的算法也可以从不同的基准测试（Liu等人，2023；Abdulhai等人，2023；Sheng等人，2023）中受益，这些基准测试试图纳入语言反馈来开发RL策略。我们正在积极在这些基准测试上测试我们的算法。

- **大语言模型作为评估函数**：我们的语言价值函数也与自然语言处理领域中一些利用大语言模型作为评估器或验证器的工作相关，包括Wang等人（2023b）、Li等人（2023）、Jiang等人（2023）、Gao等人（2024）。我们的语言价值函数与这些工作的主要区别有两点。首先，这些工作主要关注自然语言任务，而我们旨在解决决策问题。其次，这些工作中的大多数通过蒸馏强大的模型（如GPT - 4）来训练他们的大语言模型评估器，而我们主要依赖于环境反馈来生成合成评估数据。

## 6. 结论与局限性
在这项工作中，我们提出了一个全面的框架NLRL，它用自然语言表示重新制定了RL过程。我们相信NLRL为在自然语言空间中理解和实现RL算法打开了一扇新门。此外，NLRL还为生成高质量的语言合成数据提供了一种潜在途径，这对于训练更先进的语言模型可能至关重要。

我们正在努力解决以下局限性：（1）目前，大语言模型的幻觉问题严重影响了NLRL的性能。我们将专注于解决这个问题，以稳定NLRL的策略迭代过程；（2）我们的实验仅局限于表格型MDP。我们正在努力将实验扩展到表格型设置之外；（3）目前，语言价值函数和语言策略迭代过程的性能纯粹通过策略的性能来衡量。我们正在研究一种新的度量标准来全面评估性能。

### 参考文献

- 标题：NATURAL LANGUAGE REINFORCEMENT LEARNING
- 作者：Xidong Feng，Ziyu Wan，Mengyue Yang，Ziyan Wang，Girish A. Koushik，Yali Du，Ying Wen，Jun Wang
- 单位：1University College London, 2Shanghai Jiao Tong University, 3King’s Collge London, 4University of Surrey
- 标签：自然语言强化学习、强化学习、大语言模型
- 概述: 本文提出自然语言强化学习（NLRL）框架，将强化学习原理与自然语言表示结合，通过在自然语言空间重新定义RL概念，利用大语言模型实现，实验证明其在表格型MDP上的有效性、高效性和可解释性。
- 链接：https://arxiv.org/pdf/2402.07157v2
- 标题翻译：自然语言强化学习
- 摘要: 强化学习（RL）虽在决策任务策略学习方面能力卓越，但面临样本效率低、缺乏可解释性和监督信号稀疏等问题。本文受人类学习过程启发，引入自然语言强化学习（NLRL）框架，将RL概念转化为自然语言形式，重新定义任务目标、策略等。利用大语言模型实现该框架，包括作为策略、信息提取器等。在表格型MDP上的实验表明，NLRL在语言策略评估和改进方面表现出有效性，不过也存在大语言模型幻觉影响性能等局限，未来将进一步研究改进。
- 介绍: 强化学习在决策任务中有不错表现，但存在不少问题。比如它学东西慢、决策过程让人难以理解、监督信号少。而人类用自然语言学习时，能利用已有知识提高效率、能解释自己的想法、语言里的信息也更丰富。所以本文提出自然语言强化学习框架，把强化学习和自然语言结合起来，希望能更好地解决问题，还能为理解和实现强化学习算法提供新方式，也为生成高质量语言数据提供可能，对训练更好的语言模型有帮助。
