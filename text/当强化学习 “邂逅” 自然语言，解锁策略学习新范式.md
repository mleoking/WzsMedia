# 当强化学习 “邂逅” 自然语言，解锁策略学习新范式

在人工智能领域，强化学习虽取得不少成果，但也问题多多。比如它学习新知识时特“笨”，得大量样本才懂，而且做事方法让人摸不着头脑，难以理解为啥这么决策，监督信号也不给力，太稀疏了。这就导致它训练时晃晃悠悠不稳定。为解决这些，研究者从人类学习找灵感。人类用自然语言学习效率高、决策过程能说清，所以把自然语言和强化学习结合搞出了 NLRL。这个新框架能让强化学习更聪明、更透明，为未来智能决策发展指了条明路，有望大幅提升强化学习应用效果和适用范围，让人工智能更好服务生活与工作。

## 摘要 & 解读
强化学习（Reinforcement Learning，RL）在学习决策任务策略方面展现出卓越能力。然而，RL 常受样本效率低、缺乏可解释性和监督信号稀疏等问题困扰。为克服这些局限，我们从人类学习过程汲取灵感，引入自然语言强化学习（Natural Language Reinforcement Learning，NLRL），它将 RL 原理与自然语言表征创新融合。具体而言，NLRL 在自然语言空间重新定义任务目标、策略、价值函数、贝尔曼方程和策略迭代等 RL 概念。我们展示借助如 GPT - 4 等大型语言模型（Large Language Models，LLMs）的最新进展如何实际实现 NLRL。在表格马尔可夫决策过程（Markov Decision Processes，MDPs）上的初步实验证明了 NLRL 框架的有效性、效率和可解释性。

### 研究背景
强化学习构建数学框架处理决策任务，但存在样本效率低、可解释性差及监督信号稀疏致训练不稳定等问题。与之相比，人类用自然语言学习时，能凭借文本先验知识提升样本效率，决策前用语言阐述战略思维使过程可解释，且自然语言数据信息密度高。在此背景下，为克服 RL 局限、模拟人类学习优势，开展自然语言强化学习研究。
### 技术创新
- 提出自然语言强化学习（NLRL）框架，创新地将强化学习关键概念转化为自然语言表达形式，涵盖任务目标、策略、价值函数、贝尔曼方程及广义策略迭代等核心要素的重新定义，构建全新决策范式。
- 实现基于大型语言模型（LLMs）的 NLRL 技术路径，挖掘 LLMs 在策略制定、信息提取聚合、价值函数近似及策略改进等多环节的关键作用，为 NLRL 落地提供可行方案，有效增强系统可解释性、提升样本利用效率及优化决策逻辑。
### 实现设计
- 构建基于文本的马尔可夫决策过程（MDP），以自然语言描述状态、动作与环境反馈，设定语言任务指令及度量函数，明确 NLRL 优化目标为提升轨迹分布语言描述对任务指令的完成度。
- 设计语言策略，借语言描述智能体思维过程推导动作概率；定义语言价值函数，依自然语言评估策略达成任务效能；构建语言贝尔曼方程，基于状态分解直觉聚合中间变化与未来评估信息，支撑决策评估。
- 利用 LLMs 实现语言策略评估（蒙特卡洛与时间差分估计）及改进（基于任务完成度与语言分析优化），通过合理设置指令与功能定位，让 LLMs 充当策略制定者、信息处理器及决策优化推动者，保障 NLRL 框架稳定高效运行。
### 实验结果
- **文本网格世界实验**：在最短路径查找任务中，运用语言策略评估随机策略，经 GPT - 4 迭代处理，精准识别各状态最优动作，仅四次迭代就实现高效准确评估，且揭示目标信息传递规律，证明 NLRL 策略评估高效性与智能体策略优化能力。
- **随机冰湖环境实验**：四轮语言策略迭代实验显示，借助预定义概念聚合信息，策略值总体上升，验证语言策略迭代有效性。但因 GPT - 4 存在幻觉问题，系统仅达最优策略 60%效率，后续将改进提示策略提升稳定性与性能。

## 1. 引言
强化学习构建了一个数学框架，囊括关键决策要素。它通过累积奖励概念量化任务目标，用概率分布制定策略，借数学期望表达价值函数，并经状态转移和奖励函数模拟环境动态。此框架有效地将策略学习问题转化为优化问题。

尽管 RL 近年成就斐然，但其框架局限性的重大挑战依然突出。例如，RL 面临样本效率难题——RL 算法与任务无关，不利用先验知识，需大规模广泛采样来了解环境。RL 也缺乏可解释性。尽管像 AlphaZero（Silver 等人，2017）这样的模型在掌握围棋等复杂游戏时表现超人，但专业玩家也难以洞悉其决策过程的潜在战略逻辑。此外，RL 的监督信号是一维标量值，相较于文本和图像等信息丰富数据集的传统监督学习，要稀疏得多。这也是 RL 训练不稳定的原因之一（Zheng 等人，2023；Andrychowicz 等人，2020）。

这些局限促使我们构建一个受人类学习过程启发的新框架。人类在做决策时，倾向于用自然语言进行相对模糊的操作，而非像 RL 算法那样对决策组件进行数学建模。首先，自然语言赋予人类基于文本的先验知识，大幅提升学习新任务的样本效率。其次，人类独具在行动前用自然语言阐明明确战略推理和想法的能力，使决策过程对他人完全可解释，即便这并非完成任务的最有效方式。最后，自然语言数据蕴含思维、分析、评估和未来规划信息，能提供高信息密度信号，远超传统 RL 奖励信号。

受人类学习启发，我们提出自然语言强化学习（NLRL），这一全新 RL 范式创新结合传统 RL 概念与自然语言表征。通过将任务目标、策略、价值函数、贝尔曼方程和广义策略迭代（Generalized Policy Iteration，GPI）（Sutton 和 Barto，2018）等关键 RL 组件转化为自然语言等价物，我们利用语言的直观力量封装复杂决策过程。这一转化借助大型语言模型（LLMs）近期突破得以实现，LLMs 具备理解、处理和生成语言信息的类人能力。我们在表格 MDPs 上的初步实验验证了 NLRL 框架的有效性、效率和可解释性。

## 2. 强化学习预备知识
强化学习将决策问题建模为马尔可夫决策过程（MDP），由状态空间$S$、动作空间$A$、概率转移函数$P: S\times A\times S\rightarrow [0,1]$、折扣因子$\gamma\in[0,1)$和奖励函数$r: S\times A\rightarrow [-R_{max},R_{max}]$定义。RL 的目标是学习策略$\pi: S\times A\rightarrow [0,1]$，衡量给定状态$s$下动作$a$的概率：$\pi(a|s)=Pr(A_{t}=a|S_{t}=s)$。在决策任务中，最优策略倾向于使期望折扣累积奖励最大化：$\mathbb{E}_{\pi}[\sum_{t = 0}^{\infty}\gamma^{t}r(s_{t},a_{t})]$。状态 - 动作和状态价值函数是通过 RL 目标代理评估状态和动作的两个关键概念：$Q_{\pi}(s_{t},a_{t})=\mathbb{E}_{(s,a)_{t + 1:\infty}~P_{\pi}}[\sum_{i = t}^{\infty}\gamma^{i - t}r(s_{i},a_{i})|s_{t},a_{t}]$，$V_{\pi}(s_{t})=\mathbb{E}_{(s,a)_{t + 1:\infty}~P_{\pi}}[\sum_{i = t}^{\infty}\gamma^{i - t}r(s_{i},a_{i})|s_{t}]$，其中$P_{\pi}$是给定策略$\pi$和动态转移$P$的轨迹分布。

由$V_{\pi}(s_{t})$定义可推导出时间相邻状态值的关系，即贝尔曼期望方程。以下是一步贝尔曼期望方程：

$V_{\pi}(s_{t})=\mathbb{E}_{a_{t}\sim\pi_{\theta}}[r(s_{t},a_{t})+\gamma\mathbb{E}_{s_{t + 1}\sim p(s_{t + 1}|s_{t},a_{t})}[V_{\pi}(s_{t + 1})]],\forall s_{t}\in\mathcal{S}$  （1）

类似方程也可推导出$Q_{\pi}(s,a)$。基于这些基本 RL 定义和方程，我们可说明广义策略迭代（GPI）中策略评估和策略改进的实施方式。

### 2.1 策略评估
策略评估过程旨在估计状态价值函数$V_{\pi}(s)$或状态 - 动作价值函数$Q_{\pi}(s,a)$。为简化，以下说明仅用$V_{\pi}(s)$。两种常见价值函数估计方法是蒙特卡洛（Monte Carlo，MC）估计和时间差分（Temporal - Difference，TD）估计（Sutton，1988）。MC 估计利用轨迹的蒙特卡洛采样构建无偏估计：$V_{\pi}(s_{t})\approx\frac{1}{K}\sum_{n = 1}^{K}[\sum_{i = t}^{\infty}\gamma^{i - t}r(s_{i}^{n},a_{i}^{n})]$。TD 估计依据公式（1）所示时间关系构建估计：$V_{\pi}(s_{t})\approx\frac{1}{K}\sum_{n = 1}^{K}[r(s_{t},a_{t}^{n})+\gamma V_{\pi}(s_{t + 1}^{n})]$，可视为对下一状态价值函数的自举（bootstrap）。

### 2.2 策略改进
策略改进过程旨在依策略评估结果更新改进策略。具体而言，用新策略$\pi_{new}$替换旧策略$\pi_{old}$使期望回报增加：$V_{\pi_{new}}(s_{0})\geq V_{\pi_{old}}(s_{0})$。在小离散动作空间环境中，可通过在各状态贪婪选择使$Q_{\pi_{old}}(s,a)$最大的动作实现改进：

$\pi_{new}(\cdot|s)=\underset{\pi(\cdot|s)\in\mathcal{P}(\mathcal{A})}{\arg\max}\mathbb{E}_{a\sim\overline{\pi}}[Q_{\pi_{old}}(s,a)],\forall s\in\mathcal{S}$  （2）

另一种改进方法是应用策略梯度上升（Sutton 等人，1999）。用$\theta$参数化策略$\pi_{\theta}$，解析策略梯度推导如下：

$\left.\nabla_{\theta}V(\pi_{\theta})\right|_{\theta=\theta_{old}}=\mathbb{E}_{(s,a)\sim P_{\pi_{\theta_{old}}}}[\left.\nabla_{\theta}\log\pi_{\theta}(a|s)Q_{\pi_{\theta_{old}}}(s,a)\right|_{\theta=\theta_{old}}]$  （3）

选择相对小步长$\alpha>0$进行梯度上升：$\theta_{new}=\theta+\alpha\nabla_{\theta}V_{\pi_{\theta}}(s_{0})|_{\theta=\theta_{old}}$，可保证策略改进：$V_{\pi_{new}}(s_{0})\geq V_{\pi_{old}}(s_{0})$。

## 3. 自然语言强化学习
与传统 RL 所用精确统计模型不同，人类通常用自然语言构建任务目标、价值评估和战略政策等所有元素。本节旨在模拟人类用自然语言处理决策任务的方式，使其与传统 RL 的概念、定义和方程一致。因自然语言固有模糊性，此处方程并非严格依数学定义推导，而是类比并基于对原始 RL 概念的实证洞察。严格理论留待未来研究。

### 3.1 定义
我们从自然语言 RL 的定义和方程入手对人类行为建模。图 1 给出说明性示例，涵盖本节讨论的多数概念。

**基于文本的 MDP**：为在自然语言空间进行 RL，需将传统 MDP 转换为基于文本的 MDP，用文本和语言描述表示 MDP 基本概念，包括状态、动作和环境反馈（状态转移和奖励）。

**语言任务指令**：人类常定义自然语言任务指令$T_{L}$，如“达成目标”或“开门”。设人类度量$F$依轨迹描述$D_{L}(\tau_{\pi})$度量任务指令完成度，语言描述符$D_{L}$可将轨迹分布$\tau_{\pi}$转换为对应语言描述$D_{L}(\tau_{\pi})$。据此，NLRL 目标重写为：

$\max_{\pi}F(D_{L}(\tau_{\pi}),T_{L})$  （4）

即优化策略使轨迹分布$\tau_{\pi}$的语言描述展现任务指令高完成度。

**语言策略**：人类不直接建模动作概率，而通过战略思维、逻辑推理和规划确定动作。因此，用$\pi_{L}(a,c|s)$表示语言策略，先生成思维过程$c$，再得最终动作概率$\pi(a|s)$。

**语言价值函数**：类似传统 RL 中$Q$和$V$定义，人类用语言价值函数依自然语言评估策略有效性。语言状态价值函数$V_{\pi}^{L}$和语言状态 - 动作价值函数$Q_{\pi}^{L}$定义为：

$Q_{\pi}^{L}(s_{t},a_{t})=D((s,a)_{t + 1:\infty}\sim P_{\pi}|s_{t},a_{t},T_{L})$，$V_{\pi}^{L}(s_{t})=D(a_{t},(s,a)_{t + 1:\infty}\sim P_{\pi}|s_{t},T_{L})$  （5）

给定当前状态$s_{t}$或状态 - 动作$(s_{t},a_{t})$，$Q_{\pi}^{L}$和$V_{\pi}^{L}$用语言描述而非标量值展示策略达成任务目标$T_{L}$的有效性。语言价值函数富含价值信息，增强可解释性，不像传统基于标量的价值函数。它可从不同视角呈现评估结果，涵盖潜在逻辑/思维、未来结果预测/分析、不同动作比较等。

**语言贝尔曼方程**：在贝尔曼方程中，状态评估值$V(s_{t})$可分解为两部分。一是中间变化，含即时动作$a_{t}$、奖励$r_{t}$和下一状态$s_{t + 1}$；二是下一状态$s_{t + 1}$的状态评估$V(s_{t + 1})$。依此分解直觉，依分解原则引入语言贝尔曼方程（公式 6）：

$V_{\pi}^{L}(s_{t})=G_{1}^{a_{t},s_{t + 1}\sim P_{\pi}}(G_{2}(d(a_{t},r(s_{t},a_{t}),s_{t + 1}),V_{\pi}^{L}(s_{t + 1}))),\forall s_{t}\in\mathcal{S}$  （6）

其中$d(a_{t},r(s_{t},a_{t}),s_{t + 1})$描述中间变化语言描述，$G_{1}$和$G_{2}$是两个信息聚合函数。具体而言，$G_{2}$模拟原贝尔曼方程中“$+$”运算，聚合中间变化描述与给定$a_{t}$和$s_{t + 1}$的未来评估信息。$G_{1}$承担期望运算$\mathbb{E}$职责，从轨迹分布$P_{\pi}$采样不同$(a_{t},s_{t + 1})$对聚合信息。

### 3.2 语言广义策略迭代
基于上述定义和方程，介绍语言 GPI 实施方式。图 1 给出语言 GPI 说明性示例。

#### 3.2.1 语言策略评估
语言策略评估旨在估计各状态的语言价值函数$V_{\pi}^{L}$和$Q_{\pi}^{L}$。展示两种经典估计（MC 和 TD 估计）在语言策略评估中的作用。

**语言蒙特卡洛估计**：从状态$s_{t}$出发，对给定策略$\pi$的文本轨迹展开（即$K$条完整轨迹$\{a_{t},(s,a)_{t + 1:\infty}\}$）进行 MC 估计。因在语言空间不能平均运算，用语言聚合器/描述符$G_{1}$聚合有限轨迹信息近似期望评估：

$V_{\pi}^{L}(s_{t})\approx G_{1}(\{a_{t}^{n},(s,a)_{t + 1:\infty}^{n}\}_{n = 1}^{K})$  （7）

**语言时间差分估计**：TD 估计主要依据公式 6 所示一步语言贝尔曼方程。与 MC 估计类似，聚合$K$个一步样本近似期望评估：

$V_{\pi}^{L}(s_{t})\approx G_{1}(\{G_{2}(d(s_{t},a_{t}^{n},r(s_{t},a_{t}^{n}),s_{t + 1}^{n}),V_{\pi}^{L}(s_{t + 1}^{n}))\}_{n = 1}^{K}),\forall s_{t}\in\mathcal{S}$  （8）

语言 MC 估计无估计“偏差”，因直接用完整轨迹样本。但因长期未来步变化大，易出现高“方差”。此变异性使公式 7 中语言聚合器$D$难从不同轨迹提取关键信息。相反，虽下一状态评估$V_{\pi}^{L}(s_{t + 1})$不准确会给 TD 估计带来估计“偏差”，但通过舍弃未来变化有效降低“方差”。$G_{1}$和$G_{2}$只需对变化有限的一步信息简单聚合。

#### 3.2.2 语言策略改进
与传统策略改进类似，语言策略改进旨在选择使人类任务完成度函数$F$最大的动作：

$\pi_{new}(\cdot|s)=\underset{\pi(\cdot|s)\in\mathcal{P}(\mathcal{A})}{\arg\max}F(Q_{\pi_{old}}^{L}(s,a),T_{L}),\forall s\in\mathcal{S}$  （9）

如前所述，$F$是人类对任务完成度的度量，难量化和取$\arg\max$运算。因$F$很大程度依赖人类先验知识，不用数学优化，而用语言分析过程$I$进行策略优化选动作：

$\pi_{new}(\cdot|s),c = I(Q_{\pi_{old}}^{L}(s,a),T_{L}),\overline{\pi}(\cdot|s)\in\mathcal{P}(\mathcal{A}),\forall s\in\mathcal{S}$  （10）

语言策略改进经战略分析$c$生成思维过程$c$，确定最有前景动作作新策略$\pi_{new}(\cdot|s)$。此分析主要基于人类对语言评估$Q_{\pi_{old}}^{L}(s,a)$与任务目标$T_{L}$的相关性判断。

### 3.3 用大型语言模型的实际实现
第 3 节展示 NLRL 理念：将 RL 关键概念转换为人类自然语言对应物。为实际实现这些关键概念，需能理解、处理和生成语言信息的模型。经大规模人类语言和知识语料训练的大型语言模型，是模拟人类行为实现语言 RL 组件的自然选择。

- **LLMs 作策略$(\pi_{L})$**：许多工作用 LLMs 作决策智能体（Wang 等人，2023a；Feng 等人，2023a；Christianos 等人，2023；Yao 等人，2022）并结合思维链过程（Wei 等人，2022b）。设适当指令，LLMs 可用自然语言描述确定动作的潜在思维，类似人类战略思维。
- **LLMs 作信息提取器和聚合器$(G_{1},G_{2})$用于概念**：LLMs 是强大信息总结器（Zhang 等人，2023）、提取器（Xu 等人，2023）和聚合器，助我们融合语言价值函数估计的中间变化和未来语言评估。核心是确定让 LLMs 提取和聚合的信息类型。受可解释 RL 领域工作（Das 等人，2023；Sreedharan 等人，2020；Schut 等人，2023；Hayes & Shah，2017）启发，认为概念是核心。采用 Das 等人（2023）说明，概念是基于人类领域知识、面向任务目标的通用高层抽象。如最短路径查找问题中，路径距离和可用路径集是概念，是轨迹高层抽象、预定义于人类先验知识、与最终任务目标直接相关。据此，LLMs 聚合提取领域特定概念形成价值目标信息，这些概念可由人类先验知识预定义，也可由 LLMs 自身提出。

- **LLMs 作价值函数近似器$(D_{L},Q^{L},V^{L})$**：价值函数近似（Sutton 等人，1999）关键是以参数化函数替代表格表示价值函数。如今深度 RL 常选以状态为输入、输出一维标量值的神经网络。对 NLRL，$D_{L}$、$Q^{L}$、$V^{L}$的语言价值函数近似可由（多模态）LLMs 处理。LLMs 接收任务状态特征（如低维统计量、文本和图像），输出相应语言价值判断和描述。LLMs 训练采用上述概念提取器/聚合器形成 MC 或 TD 估计（见 3.2.1 节），微调 LLMs 提升语言评估能力。
- **LLMs 作策略改进算子$(I)$**：借助思维链过程和人类先验知识，LLMs 通过对语言评估$Q_{\pi_{old}}^{L}(s,a)$与任务目标$T_{L}$相关性的语言分析$c$，更好确定最有前景动作$\pi_{new}(\cdot|s)$。此理念与近期用 LLMs 或视觉语言模型作奖励的工作（Kwon 等人，2023；Rocamonde 等人，2023）一致，可准确模拟相关性。

### 3.4 其他 RL 概念讨论
为说明框架通用性，展示几个将其他基本 RL 概念纳入 NLRL 的例子。

- **TD - λ（Sutton，1988）**：公式 6 考虑价值函数一步分解，即传统 RL 中 TD(1)情形。自然扩展是对语言价值函数进行$n$步扩展形成 TD(n)估计，进一步聚合 1 到$n$步不同估计得 NLRL 的 TD - λ估计。TD - λ在语言价值函数中是将不同语言评估聚合成最终语言估计。
- **探索**：探索是 RL 基本挑战。传统 RL 方法常用引入随机性策略（如$\epsilon$-贪婪法或熵正则化（Sutton & Barto，2018））促进探索。相比之下，NLRL 提供基于不确定性引导的探索途径。如 NLRL 语言价值函数擅纳入状态不确定性模型，语言策略改进时可深入分析优化策略减少不确定性。
- **情节和基于检索的 RL**：情节和基于检索的 RL（Goyal 等人，2022；Blundell 等人，2016）概念是显式记录过去轨迹用于增强新状态决策能力，与 LLMs 中检索增强生成（RAG）（Lewis 等人，2020）理念契合。NLRL 借助 RAG 利用过去轨迹扩展为基于检索的 RL。

## 4. 实验
在表格 MDP 上进行初步实验作概念验证。更多实验结果、设置和提示见附录 A。

### 4.1 热身：文本网格世界中的语言策略评估
从最短路径查找问题入手，常用作动态规划策略迭代算法示例。如图 3（迭代 0）所示，这是$4\times4$网格世界，目标是以最短路径到达$(0,0)$和$(3,3)$的灰色网格。将此环境转为文本游戏，用自然语言表示状态、转移和环境反馈（路径惩罚和到达目标终止）。设初始策略为从所有可用动作均匀分布采样的随机策略：$\pi(a|s)=\frac{1}{|A|},\forall a\in A,s\in S$。对该策略进行语言策略评估，类似原策略迭代算法中的策略评估过程。

表 1：预定义概念名称及对应定义
|概念名称|概念定义|
|---|---|
|重要状态|记录重要状态位置，如目标位置和洞位置|
|即时风险|识别即时一步失败|
|未来风险|识别潜在未来失败路径（多于一步）或未来不良状态|
|最安全路径|从给定状态和动作生成最安全路径|
|最终评估|给定状态下策略的最终评估|

除终端状态用预定义终端性质描述初始化外，用“无评估信息”初始化语言状态价值表中所有状态的语言评估描述。与传统策略评估过程类似，每次语言策略评估遍历所有非终端状态。对各状态$s$和动作$a$，依公式 6 一步贝尔曼方程计算新语言状态 - 动作价值函数$Q^{L}(s,a)$和$V^{L}(s)$。经四次迭代观察语言状态 - 动作价值函数演变。用 GPT - 4preview - 1106 模型作语言贝尔曼更新的信息聚合器。通过将坐标转字母，确保 GPT - 4 仅依环境反馈收集信息，消除对固有知识库依赖。

图 2 展示不同迭代下状态$g(0,3)$的语言评估结果作示例。不同状态综合结果见附录 A。分析得出三个关键结论。首先，尽管提示 GPT - 4 模型作策略评估，但它固有进行策略改进过程，选择最有利动作选项，表明 GPT - 4 有语言策略增强能力，能解读语言评估和任务目标。其次，最终迭代评估结果正确识别各状态最优动作，证实语言策略评估和迭代过程的有效性与效率，仅四次策略评估迭代就得出精确语言评估和相应最优动作，远超传统策略迭代法。最后，语言评估结果阐明目标相关信息逐渐传递，图 2 示例展示目标信息经三次评估迭代流向状态$g$。图 3 直观呈现迭代间信息流动。

### 4.2 随机环境中的语言策略评估与改进
第二个实验探索 Brockman 等人（2016）提出的“冰湖”环境，这是一个网格世界，智能体需导航至目标并避开冰洞，因状态转移随机性具挑战性。依 Sheng 等人（2023）框架将此环境转为文本格式。进行四轮语言策略迭代，各轮含评估和改进阶段。

初步实验表明，首实验的直接提示法易使“冰湖”任务策略迭代失败，因轨迹变化多易分散 LLM 注意力、扰乱聚合过程。为解决此问题，采用更显式聚合方法：3.3 节讨论的基于概念的语言价值函数和信息提取。预定义表 1 中几个关键概念，提示 GPT - 4 - 0125 - preview 模型依此生成评估和聚合信息。

图 4 给出语言价值函数和语言策略改进过程示例。价值函数示例清晰说明定义任务相关概念为何利于信息聚合。GPT - 4 易进行显式概念聚合：可通过在最左边添加最新移动更新路径信息，或通过传递目标和洞坐标提取环境信息。

表 2 用“冰湖”原奖励函数给出各迭代策略值。表中呈总体上升趋势，验证语言策略迭代过程有效性。但系统仅达最优策略$60\%$效率。进一步分析语言价值函数发现问题：即使最先进的 GPT - 4 模型也受幻觉问题困扰，偶尔生成不准确信息。这些不准确信息经连续贝尔曼更新积累，致错误评估和决策。正通过改进提示策略缓解此不稳定性。

## 5. 相关工作
### 5.1 基于语言模型的自主智能体
受 LLMs 在复杂推理和规划场景强大涌现能力启发（Brown 等人，2020；Wei 等人，2022a；Anil 等人，2023；OpenAI，2023；Feng 等人，2023b），基于语言模型的自主智能体领域渐趋用 LLMs 实现高级规划目的。文本智能体中，ReAct（Yao 等人，2022）通过思维链规划中间目标、用少样本提示任务特定生成。Reflexion（Shinn 等人，2023）基于 ReAct（Yao 等人，2022）构建，有自我反思能力（称为言语强化学习），依在线反馈生成提示，与语言蒙特卡洛估计中从多轨迹采样提取核心信息/概念的设置有一定契合。Zhang 等人（2023）、Xu 等人（2023）等工作用大型语言模型总结提取信息助融合中间变化。在具身智能体研究（Duan 等人，2022）中，机器人学习研究展示大型语言模型在分层规划和子目标生成方面潜力（Huang 等人，2022；Liang 等人，2023）。

### 5.2 可解释强化学习
可解释 RL 的主要目的之一是为非 AI 专家自动寻求解释。如针对顺序决策任务的基于概念解释方法。Ji 等人（2023）通过聚类学习的人类可解释特征为 3D 动作识别 CovNets 提供概念解释。Sreedharan 等人（2020）依状态前提和动作成本构建概念解释，表示用户与给定状态关联的任何事实陈述。Hayes & Shah（2017）用逻辑公式总结策略。Das 等人（2023）训练状态 - 动作对和概念解释的联合嵌入模型。

### 5.3 从语言反馈中学习
我们的工作也与从语言反馈中学习相关（Cheng 等人，2023）。Cheng 等人（2023）关注算法基准测试，我们旨在提出新算法框架。机器人和自主智能体领域研究如何将语言反馈作为学习信号的一部分提供给 RL 智能体活动，多数努力集中于引出二元偏好反馈（Biyik & Sadigh，2018）或人类排名标签（Sumers 等人，2021）。我们的算法也可受益于不同基准测试（Liu 等人，2023；Abdulhai 等人，2023；Sheng 等人，2023），这些测试尝试纳入语言反馈开发 RL 策略，我们正积极在这些基准上测试算法。

### 5.4 LLM 作评估函数
我们的语言价值函数与 NLP 领域一些用 LLMs 作评估器或验证器的工作相关，包括 Wang 等人（2023b）；Li 等人（2023）；Jiang 等人（2023）；Gao 等人（2024）。我们语言价值函数与这些工作主要有两点区别。一是这些工作主要聚焦自然语言任务，我们旨在解决决策问题。二是这些工作多通过提炼如 GPT - 4 等强模型训练 LLM 评估器，我们主要依环境反馈生成合成评估数据。

## 6. 结论与局限
本文提出 NLRL 综合框架，用自然语言表征重构 RL 过程。我们认为 NLRL 为在自然语言空间理解和实现 RL 算法开启新门。此外，NLRL 为生成高质量语言合成数据提供潜在途径，对训练更先进语言模型至关重要。

我们正努力解决以下局限：（1）目前，LLM 的幻觉问题严重影响 NLRL 性能。我们将专注解决此问题以稳定 NLRL 策略迭代过程；（2）实验目前限于表格 MDP。我们正努力将实验扩展到表格设置之外；（3）目前，语言价值函数和语言策略迭代过程性能仅由策略性能衡量。我们正研究新指标全面评估性能。 

### 参考资料

- 标题: NATURAL LANGUAGE REINFORCEMENT LEARNING
- 作者: Xidong Feng¹∗, Ziyu Wan²∗, Mengyue Yang¹, Ziyan Wang³, Girish A. Koushik⁴, Yali Du³, Ying Wen², Jun Wang
- 单位：1伦敦大学学院，2上海交通大学，3伦敦国王学院，4萨里大学
- 标签：强化学习、自然语言处理、人工智能
- 概述：本文受人类学习过程启发提出自然语言强化学习（NLRL）框架，将 RL 原理与自然语言表征结合，重新定义 RL 概念，利用 LLMs 实现并通过表格 MDPs 实验验证其有效性、效率和可解释性，同时指出当前局限及改进方向。
- 链接：https://arxiv.org/pdf/2402.07157v2 
